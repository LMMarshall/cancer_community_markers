{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96c18f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import (LinearRegression, Ridge, Lasso, LassoCV, ElasticNet, PoissonRegressor)\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import xgboost as xg\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import keras_tuner as kt\n",
    "from sklearn.decomposition import PCA\n",
    "import warnings\n",
    "\n",
    "#load training and test data\n",
    "dataset = pd.read_csv('cleanData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38601cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A function to drop the categorical features from the dataframe\n",
    "def dropCat_o(data):\n",
    "    return data.drop(['Area Code','Area Name','parent_name','parent_area'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfdba92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A function to drop the categorical features from the dataframe as well as the features where\n",
    "#large numbers of records had a rate of zero recorded (and so will add to rather than decrease uncertainty in\n",
    "#modeling)\n",
    "def dropCat(data):\n",
    "    return data.drop(['Area Code','Area Name','parent_name','parent_area',\n",
    "                     'typhoid','measles','hepatitis','STEC','giardia','shigella','listeria','mumps','IMD'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b68d761f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into train and test sets\n",
    "train_data, test_data = train_test_split(dataset, test_size = 0.2, random_state = 43)\n",
    "train_data.to_csv(\"communities_training.csv\", index=False)\n",
    "test_data.to_csv(\"communities_testing.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c25f037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('communities_training.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb2561cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Area Code</th>\n",
       "      <th>Area Name</th>\n",
       "      <th>gen_warts</th>\n",
       "      <th>syphilis</th>\n",
       "      <th>gonorrhea</th>\n",
       "      <th>HIV</th>\n",
       "      <th>herpes</th>\n",
       "      <th>chlamydia</th>\n",
       "      <th>TB</th>\n",
       "      <th>comm_disease_flu</th>\n",
       "      <th>...</th>\n",
       "      <th>back_pain</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>MSK</th>\n",
       "      <th>walking</th>\n",
       "      <th>biking</th>\n",
       "      <th>cancer_death_rate</th>\n",
       "      <th>education</th>\n",
       "      <th>premature</th>\n",
       "      <th>parent_area</th>\n",
       "      <th>parent_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E06000018</td>\n",
       "      <td>Nottingham</td>\n",
       "      <td>72.085861</td>\n",
       "      <td>11.865986</td>\n",
       "      <td>189.559112</td>\n",
       "      <td>3.128908</td>\n",
       "      <td>34.114708</td>\n",
       "      <td>558.591248</td>\n",
       "      <td>11.780366</td>\n",
       "      <td>16.957531</td>\n",
       "      <td>...</td>\n",
       "      <td>10.990853</td>\n",
       "      <td>75.194243</td>\n",
       "      <td>14.427301</td>\n",
       "      <td>20.5955</td>\n",
       "      <td>2.4518</td>\n",
       "      <td>150.377547</td>\n",
       "      <td>44.9835</td>\n",
       "      <td>85.0307</td>\n",
       "      <td>E12000004</td>\n",
       "      <td>East Midlands</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E07000075</td>\n",
       "      <td>Rochford</td>\n",
       "      <td>36.518425</td>\n",
       "      <td>2.282402</td>\n",
       "      <td>60.483643</td>\n",
       "      <td>0.966366</td>\n",
       "      <td>21.682816</td>\n",
       "      <td>170.038910</td>\n",
       "      <td>1.151375</td>\n",
       "      <td>6.698576</td>\n",
       "      <td>...</td>\n",
       "      <td>11.248205</td>\n",
       "      <td>79.306452</td>\n",
       "      <td>21.799242</td>\n",
       "      <td>9.6329</td>\n",
       "      <td>2.2186</td>\n",
       "      <td>114.962098</td>\n",
       "      <td>61.7464</td>\n",
       "      <td>56.8720</td>\n",
       "      <td>E10000012</td>\n",
       "      <td>Essex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E07000163</td>\n",
       "      <td>Craven</td>\n",
       "      <td>24.416616</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>19.184484</td>\n",
       "      <td>0.864394</td>\n",
       "      <td>12.208308</td>\n",
       "      <td>102.898598</td>\n",
       "      <td>1.758726</td>\n",
       "      <td>4.441179</td>\n",
       "      <td>...</td>\n",
       "      <td>10.794776</td>\n",
       "      <td>70.855820</td>\n",
       "      <td>21.694706</td>\n",
       "      <td>16.7428</td>\n",
       "      <td>0.7203</td>\n",
       "      <td>88.008330</td>\n",
       "      <td>65.1079</td>\n",
       "      <td>73.9927</td>\n",
       "      <td>E10000023</td>\n",
       "      <td>North Yorkshire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E08000035</td>\n",
       "      <td>Leeds</td>\n",
       "      <td>51.077511</td>\n",
       "      <td>5.758739</td>\n",
       "      <td>117.928955</td>\n",
       "      <td>2.783867</td>\n",
       "      <td>38.934082</td>\n",
       "      <td>530.680298</td>\n",
       "      <td>7.815209</td>\n",
       "      <td>11.805130</td>\n",
       "      <td>...</td>\n",
       "      <td>10.590229</td>\n",
       "      <td>77.169091</td>\n",
       "      <td>16.564005</td>\n",
       "      <td>18.2172</td>\n",
       "      <td>1.2626</td>\n",
       "      <td>133.936918</td>\n",
       "      <td>54.8378</td>\n",
       "      <td>70.0965</td>\n",
       "      <td>E12000003</td>\n",
       "      <td>Yorkshire and The Humber</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E07000121</td>\n",
       "      <td>Lancaster</td>\n",
       "      <td>53.335495</td>\n",
       "      <td>6.076196</td>\n",
       "      <td>45.909031</td>\n",
       "      <td>0.714595</td>\n",
       "      <td>43.883633</td>\n",
       "      <td>201.864716</td>\n",
       "      <td>3.003898</td>\n",
       "      <td>9.168298</td>\n",
       "      <td>...</td>\n",
       "      <td>10.392378</td>\n",
       "      <td>78.723632</td>\n",
       "      <td>17.454334</td>\n",
       "      <td>15.7211</td>\n",
       "      <td>3.1358</td>\n",
       "      <td>130.391296</td>\n",
       "      <td>58.5099</td>\n",
       "      <td>92.4350</td>\n",
       "      <td>E10000017</td>\n",
       "      <td>Lancashire</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Area Code   Area Name  gen_warts   syphilis   gonorrhea       HIV  \\\n",
       "0  E06000018  Nottingham  72.085861  11.865986  189.559112  3.128908   \n",
       "1  E07000075    Rochford  36.518425   2.282402   60.483643  0.966366   \n",
       "2  E07000163      Craven  24.416616   0.000000   19.184484  0.864394   \n",
       "3  E08000035       Leeds  51.077511   5.758739  117.928955  2.783867   \n",
       "4  E07000121   Lancaster  53.335495   6.076196   45.909031  0.714595   \n",
       "\n",
       "      herpes   chlamydia         TB  comm_disease_flu  ...  back_pain  \\\n",
       "0  34.114708  558.591248  11.780366         16.957531  ...  10.990853   \n",
       "1  21.682816  170.038910   1.151375          6.698576  ...  11.248205   \n",
       "2  12.208308  102.898598   1.758726          4.441179  ...  10.794776   \n",
       "3  38.934082  530.680298   7.815209         11.805130  ...  10.590229   \n",
       "4  43.883633  201.864716   3.003898          9.168298  ...  10.392378   \n",
       "\n",
       "    diabetes        MSK  walking  biking  cancer_death_rate  education  \\\n",
       "0  75.194243  14.427301  20.5955  2.4518         150.377547    44.9835   \n",
       "1  79.306452  21.799242   9.6329  2.2186         114.962098    61.7464   \n",
       "2  70.855820  21.694706  16.7428  0.7203          88.008330    65.1079   \n",
       "3  77.169091  16.564005  18.2172  1.2626         133.936918    54.8378   \n",
       "4  78.723632  17.454334  15.7211  3.1358         130.391296    58.5099   \n",
       "\n",
       "   premature  parent_area               parent_name  \n",
       "0    85.0307    E12000004             East Midlands  \n",
       "1    56.8720    E10000012                     Essex  \n",
       "2    73.9927    E10000023           North Yorkshire  \n",
       "3    70.0965    E12000003  Yorkshire and The Humber  \n",
       "4    92.4350    E10000017                Lancashire  \n",
       "\n",
       "[5 rows x 46 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we can see that the scale of the features varies\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "374c9e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split training data into training and validation sets\n",
    "X_train,X_validate,Y_train,Y_validate = train_test_split(\n",
    "    dropCat(train_data).drop('cancer_death_rate',axis=1),\n",
    "    train_data['cancer_death_rate'],\n",
    "    test_size = .2, \n",
    "    random_state = 43\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c503cea0",
   "metadata": {},
   "source": [
    "## Define training data\n",
    "\n",
    "Based on initial exploration of the data, I know that the features have different scales and so need to be rescaled. I also know there is a lot of skew in many of the features, so I want to test a version of the dataset where the skew has been addressed. And since I have 41 predictors, I also want to try subsetting to just the most powerful of those. So below I run through these processes and create versions of the dataset to address them. I can then use these versions when I move on to modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3040a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create scaled dataset\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled=pd.DataFrame(scaler.fit_transform(X_train),columns=X_train.columns)\n",
    "X_validate_scaled=pd.DataFrame(scaler.transform(X_validate),columns=X_validate.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d15ef40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform skew using PowerTransformer instead of custom transformer above\n",
    "pt = PowerTransformer()\n",
    "X_train_trans = pd.DataFrame(pt.fit_transform(X_train),columns=X_train.columns)\n",
    "X_validate_trans = pd.DataFrame(pt.transform(X_validate),columns=X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f13d1137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best alpha, scaling only:  0.28236808159511034\n",
      "r2:  0.6309173202687934\n",
      "             feature lasso_score abs_score\n",
      "16    cardiovascular     8.34177   8.34177\n",
      "17       respiratory     5.38099   5.38099\n",
      "30         education    -3.40583   3.40583\n",
      "19        indoor_air    -2.83646   2.83646\n",
      "5          chlamydia     2.39923   2.39923\n",
      "6                 TB    -2.07257   2.07257\n",
      "15      low_birth_wt    -2.01953   2.01953\n",
      "12          pertusis     1.53905   1.53905\n",
      "26          diabetes     1.46729   1.46729\n",
      "22         pollution     -1.4468    1.4468\n",
      "9         salmonella     1.40147   1.40147\n",
      "14             noise    -1.33016   1.33016\n",
      "8        antibiotics     1.20621   1.20621\n",
      "18      osteoporosis    -1.18098   1.18098\n",
      "27               MSK    0.944418  0.944418\n",
      "7   comm_disease_flu    0.877433  0.877433\n",
      "13            mental    0.779527  0.779527\n",
      "29            biking    0.745473  0.745473\n",
      "21           alcohol    0.532981  0.532981\n",
      "11   cryptosporidium   -0.495398  0.495398\n",
      "20                RA   -0.279779  0.279779\n",
      "best alpha, scaling plus tranform for skew:  0.24379423487066268\n",
      "r2:  0.6528496725529129\n",
      "             feature lasso_score abs_score\n",
      "16    cardiovascular     7.61693   7.61693\n",
      "17       respiratory     6.42568   6.42568\n",
      "30         education    -2.83808   2.83808\n",
      "27               MSK     2.78176   2.78176\n",
      "19        indoor_air    -2.74047   2.74047\n",
      "15      low_birth_wt    -2.49964   2.49964\n",
      "22         pollution    -2.49888   2.49888\n",
      "8        antibiotics     1.41255   1.41255\n",
      "20                RA    -1.40654   1.40654\n",
      "9         salmonella     1.26534   1.26534\n",
      "26          diabetes     1.09377   1.09377\n",
      "12          pertusis     1.00489   1.00489\n",
      "29            biking    0.886791  0.886791\n",
      "7   comm_disease_flu    0.849354  0.849354\n",
      "6                 TB     0.83138   0.83138\n",
      "5          chlamydia     0.56336   0.56336\n",
      "13            mental    0.540307  0.540307\n",
      "14             noise   -0.516822  0.516822\n",
      "31         premature   -0.427022  0.427022\n",
      "3                HIV    0.366887  0.366887\n",
      "18      osteoporosis   -0.361101  0.361101\n",
      "1           syphilis    0.312349  0.312349\n",
      "21           alcohol    0.165136  0.165136\n",
      "23          inactive   -0.146118  0.146118\n"
     ]
    }
   ],
   "source": [
    "#Create version subsetted to the most predictive features after finding those features via Lasso\n",
    "lassoCV = LassoCV()\n",
    "lassoCV.fit(X_train_scaled,Y_train)\n",
    "lasso_predict = lassoCV.predict(X_validate_scaled)\n",
    "best_alpha = lassoCV.alpha_\n",
    "coefs = lassoCV.coef_\n",
    "table1 = [X_train.columns, coefs]\n",
    "table1 = pd.DataFrame(table1).T.rename(columns={0:'feature',1:'lasso_score'})\n",
    "table1['abs_score'] = abs(table1['lasso_score'])\n",
    "table1 = table1[table1.abs_score>0]\n",
    "table1.sort_values(by='abs_score', ascending=False,inplace=True)\n",
    "print('best alpha, scaling only: ',best_alpha)\n",
    "lasso = Lasso(alpha=best_alpha)\n",
    "lasso.fit(X_train_scaled,Y_train) \n",
    "lasso_predict = lasso.predict(X_validate_scaled)\n",
    "r2 = r2_score(Y_validate, lasso_predict)\n",
    "print('r2: ',r2)\n",
    "print(table1)\n",
    "#------------------\n",
    "lassoCVtrans = LassoCV()\n",
    "lassoCVtrans.fit(X_train_trans,Y_train)\n",
    "lasso_predict_trans = lassoCVtrans.predict(X_validate_trans)\n",
    "best_alpha_trans = lassoCVtrans.alpha_\n",
    "coefs2 = lassoCVtrans.coef_\n",
    "table2 = [X_train.columns, coefs2]\n",
    "table2 = pd.DataFrame(table2).T.rename(columns={0:'feature',1:'lasso_score'})\n",
    "table2['abs_score'] = abs(table2['lasso_score'])\n",
    "table2 = table2[table2.abs_score>0]\n",
    "table2.sort_values(by='abs_score', ascending=False,inplace=True)\n",
    "print('best alpha, scaling plus tranform for skew: ',best_alpha_trans)\n",
    "lasso_trans = Lasso(alpha=best_alpha_trans)\n",
    "lasso_trans.fit(X_train_trans,Y_train) \n",
    "lasso_predict_trans = lasso_trans.predict(X_validate_trans)\n",
    "r2_trans = r2_score(Y_validate, lasso_predict_trans)\n",
    "print('r2: ',r2_trans)\n",
    "print(table2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f863de8",
   "metadata": {},
   "source": [
    "From the above, we can draw 2 conclusions:  \n",
    "1. There is a subset of the 41 features that appears to hold all of the predictive power.\n",
    "2. Correcting for skew in the data results in a poorer model.\n",
    "I'll create a subsetted version of the data (not corrected for skew) to try out in the modelling process along with the full feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5c3164f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create subset of data using the Lasso-selected features from the transformed dataset\n",
    "X_train_subset_scaled = X_train_trans[list(table2['feature'])]\n",
    "X_validate_subset_scaled = X_validate_trans[list(table2['feature'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34db9718",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376486c7",
   "metadata": {},
   "source": [
    "Regresion models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20ccb48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_types = [LinearRegression(), Ridge(), Lasso(), LassoCV(), ElasticNet(), PoissonRegressor()]\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "models = ['LR scaled','LR subset','LR trans', \n",
    "          'Ridge scaled','Ridge subset','Ridge trans',\n",
    "         'Lasso scaled','Lasso subset','Lasso trans',\n",
    "         'LassoCV scaled','LassoCV subset','LassoCV trans',\n",
    "         'EN scaled','EN subset','EN trans',\n",
    "         'Pois scaled', 'Pois subset', 'Pois trans']\n",
    "r2s = []\n",
    "MSEs = []\n",
    "\n",
    "for model in model_types:\n",
    "    for dataset in [[X_train_scaled, X_validate_scaled],\n",
    "                    [X_train_subset_scaled,X_validate_subset_scaled],\n",
    "                    [X_train_trans,X_validate_trans]]:\n",
    "        model.fit(dataset[0],Y_train)\n",
    "        mod_pred = model.predict(dataset[1])\n",
    "        r2s.append(r2_score(Y_validate, mod_pred))\n",
    "        MSEs.append(mean_squared_error(Y_validate, mod_pred))\n",
    "\n",
    "tableData = [models,MSEs,r2s]\n",
    "regResults = pd.DataFrame(tableData).T\n",
    "regResults.columns=['model','MSE','r2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d50efa4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>MSE</th>\n",
       "      <th>r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LassoCV trans</td>\n",
       "      <td>138.093</td>\n",
       "      <td>0.65285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Pois subset</td>\n",
       "      <td>143.567</td>\n",
       "      <td>0.639089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LassoCV subset</td>\n",
       "      <td>143.782</td>\n",
       "      <td>0.638547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ridge subset</td>\n",
       "      <td>145.256</td>\n",
       "      <td>0.634842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Pois trans</td>\n",
       "      <td>145.654</td>\n",
       "      <td>0.633842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LR subset</td>\n",
       "      <td>146.018</td>\n",
       "      <td>0.632925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LassoCV scaled</td>\n",
       "      <td>146.817</td>\n",
       "      <td>0.630917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ridge trans</td>\n",
       "      <td>148.247</td>\n",
       "      <td>0.627322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LR trans</td>\n",
       "      <td>149.212</td>\n",
       "      <td>0.624897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Lasso subset</td>\n",
       "      <td>149.566</td>\n",
       "      <td>0.624006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Lasso trans</td>\n",
       "      <td>149.577</td>\n",
       "      <td>0.623979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>EN subset</td>\n",
       "      <td>154.047</td>\n",
       "      <td>0.612742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Lasso scaled</td>\n",
       "      <td>156.029</td>\n",
       "      <td>0.60776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>EN trans</td>\n",
       "      <td>156.183</td>\n",
       "      <td>0.607371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>EN scaled</td>\n",
       "      <td>165.185</td>\n",
       "      <td>0.584743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ridge scaled</td>\n",
       "      <td>167.49</td>\n",
       "      <td>0.578948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR scaled</td>\n",
       "      <td>168.062</td>\n",
       "      <td>0.577511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Pois scaled</td>\n",
       "      <td>171.763</td>\n",
       "      <td>0.568207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             model      MSE        r2\n",
       "11   LassoCV trans  138.093   0.65285\n",
       "16     Pois subset  143.567  0.639089\n",
       "10  LassoCV subset  143.782  0.638547\n",
       "4     Ridge subset  145.256  0.634842\n",
       "17      Pois trans  145.654  0.633842\n",
       "1        LR subset  146.018  0.632925\n",
       "9   LassoCV scaled  146.817  0.630917\n",
       "5      Ridge trans  148.247  0.627322\n",
       "2         LR trans  149.212  0.624897\n",
       "7     Lasso subset  149.566  0.624006\n",
       "8      Lasso trans  149.577  0.623979\n",
       "13       EN subset  154.047  0.612742\n",
       "6     Lasso scaled  156.029   0.60776\n",
       "14        EN trans  156.183  0.607371\n",
       "12       EN scaled  165.185  0.584743\n",
       "3     Ridge scaled   167.49  0.578948\n",
       "0        LR scaled  168.062  0.577511\n",
       "15     Pois scaled  171.763  0.568207"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regResults.sort_values(by='r2',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6976ec",
   "metadata": {},
   "source": [
    "XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8074462c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getXGparams(training_X, training_Y, objective='reg:squarederror'):\n",
    "    param_options = {'max_depth':[2,3,4],\n",
    "                    'learning_rate': [.17,.13,.1,.05,.02],\n",
    "                    'gamma': [0,.25,.5]}\n",
    "\n",
    "    #find best params using 90% of data on each iteration and 50% of cols per tree\n",
    "    find_best = GridSearchCV(\n",
    "        estimator = xg.XGBRegressor(objective=objective,\n",
    "                                    seed = 43,\n",
    "                                   subsample=0.9,\n",
    "                                   colsample_bytree=0.5),\n",
    "        param_grid = param_options,\n",
    "        cv=3\n",
    "    )\n",
    "\n",
    "    find_best.fit(training_X,training_Y)\n",
    "\n",
    "    return [find_best.best_params_['gamma'],find_best.best_params_['learning_rate'],find_best.best_params_['max_depth']]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ddfa98fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runXGmodel(gamma, learning_rate, max_depth, training_X, training_Y, validate_X, validate_Y,\n",
    "              objective='reg:squarederror'):\n",
    "    xgbModel = xg.XGBRegressor(objective=objective,\n",
    "                               seed = 43,\n",
    "                              gamma=gamma,\n",
    "                              learning_rate=learning_rate,\n",
    "                              max_depth=max_depth)\n",
    "    xgbModel.fit(training_X, training_Y)\n",
    "    pred=xgbModel.predict(validate_X)\n",
    "    error = mean_squared_error(validate_Y, pred)\n",
    "    r2 = r2_score(validate_Y,pred)\n",
    "    return(error, r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f6aef527",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sets = ['reg_scaled','reg_subset+scaled','reg_tranformed+scaled',\n",
    "             'pois_scaled','pois_subset+scaled','pois_tranformed+scaled']\n",
    "errors = []\n",
    "r2s = []\n",
    "for dataset in [[X_train_scaled, X_validate_scaled,'reg:squarederror'], \n",
    "                [X_train_subset_scaled, X_validate_subset_scaled,'reg:squarederror'],\n",
    "                [X_train_trans, X_validate_trans,'reg:squarederror'],\n",
    "                [X_train_scaled, X_validate_scaled,'count:poisson'], \n",
    "                [X_train_subset_scaled, X_validate_subset_scaled,'count:poisson'],\n",
    "                [X_train_trans, X_validate_trans,'count:poisson']]:\n",
    "    gamma, lr, md = getXGparams(dataset[0],Y_train, dataset[2])\n",
    "    error, r2 = runXGmodel(gamma, lr, md, dataset[0],Y_train, dataset[1],Y_validate, dataset[2])\n",
    "    errors.append(error)\n",
    "    r2s.append(r2)\n",
    "tableXGresults = [data_sets,errors,r2s]\n",
    "tableXGresults = pd.DataFrame(tableXGresults).T\n",
    "tableXGresults.columns=['data_set','MSE','r2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d8b39e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Model results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_set</th>\n",
       "      <th>MSE</th>\n",
       "      <th>r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>reg_tranformed+scaled</td>\n",
       "      <td>155.267</td>\n",
       "      <td>0.609675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>reg_scaled</td>\n",
       "      <td>157.77</td>\n",
       "      <td>0.603384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>reg_subset+scaled</td>\n",
       "      <td>158.449</td>\n",
       "      <td>0.601676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pois_scaled</td>\n",
       "      <td>158.695</td>\n",
       "      <td>0.601056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>pois_tranformed+scaled</td>\n",
       "      <td>158.969</td>\n",
       "      <td>0.600368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pois_subset+scaled</td>\n",
       "      <td>164.381</td>\n",
       "      <td>0.586764</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 data_set      MSE        r2\n",
       "2   reg_tranformed+scaled  155.267  0.609675\n",
       "0              reg_scaled   157.77  0.603384\n",
       "1       reg_subset+scaled  158.449  0.601676\n",
       "3             pois_scaled  158.695  0.601056\n",
       "5  pois_tranformed+scaled  158.969  0.600368\n",
       "4      pois_subset+scaled  164.381  0.586764"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('XGBoost Model results:')\n",
    "tableXGresults.sort_values(by='r2',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733de7dc",
   "metadata": {},
   "source": [
    "Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a549f2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRFparams(dataset):\n",
    "    param_options = {'max_depth':[2,3,4,5,6],\n",
    "                'min_samples_leaf': [2,3,4,5]\n",
    "                }\n",
    "    #find best params using 90% of data on each iteration and 50% of cols per tree\n",
    "    find_best = GridSearchCV(\n",
    "        estimator = RandomForestRegressor(random_state=43),\n",
    "        param_grid = param_options,\n",
    "        cv=3\n",
    "    )\n",
    "\n",
    "    find_best.fit(dataset[0],Y_train)\n",
    "    return [find_best.best_params_['max_depth'],find_best.best_params_['min_samples_leaf']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fa397fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runRFmodel(max_depth, min_samples_leaf, dataset):\n",
    "    regr = RandomForestRegressor(max_depth=max_depth, min_samples_leaf=min_samples_leaf, random_state=43)\n",
    "    regr.fit(dataset[0], Y_train)\n",
    "\n",
    "    regr_predict = regr.predict(dataset[1])\n",
    "    error = mean_squared_error(Y_validate, regr_predict)\n",
    "    r2 = r2_score(Y_validate, regr_predict)\n",
    "    return [error, r2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7202a8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sets = ['scaled','subset+scaled','trans+scaled']\n",
    "errors = []\n",
    "r2s = []\n",
    "for dataset in [[X_train_scaled, X_validate_scaled], [X_train_subset_scaled, X_validate_subset_scaled],\n",
    "               [X_train_trans, X_validate_trans]]:\n",
    "    max_depth, min_samples_leaf = getRFparams(dataset)\n",
    "    error, r2 = runRFmodel(max_depth, min_samples_leaf, dataset)\n",
    "    errors.append(error)\n",
    "    r2s.append(r2)\n",
    "tableRFresults = [data_sets,errors,r2s]\n",
    "tableRFresults = pd.DataFrame(tableRFresults).T\n",
    "tableRFresults.columns=['data_set','MSE','r2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "120e94b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Model results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_set</th>\n",
       "      <th>MSE</th>\n",
       "      <th>r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>subset+scaled</td>\n",
       "      <td>161.927</td>\n",
       "      <td>0.592932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>trans+scaled</td>\n",
       "      <td>164.129</td>\n",
       "      <td>0.587398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>scaled</td>\n",
       "      <td>164.147</td>\n",
       "      <td>0.587351</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        data_set      MSE        r2\n",
       "1  subset+scaled  161.927  0.592932\n",
       "2   trans+scaled  164.129  0.587398\n",
       "0         scaled  164.147  0.587351"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Random Forest Model results:')\n",
    "tableRFresults.sort_values(by='r2',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d018e2",
   "metadata": {},
   "source": [
    "To this point, the best performing model is the LassoCV regression model, which has transformed features and an r-squared of ~0.65.  \n",
    "\n",
    "Now I'll look into principal component analysis to see if I can improve predictive strength through dimensionality reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4881e4d9",
   "metadata": {},
   "source": [
    "PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f62fda0",
   "metadata": {},
   "source": [
    "Regression models with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1898d29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_types = [LinearRegression(), Ridge(), Lasso(), LassoCV(), ElasticNet(), PoissonRegressor()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d69a8c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to run PCA across several models and provide the score, given a training and validation set\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "def getPCAresults(dataset):\n",
    "    models = []\n",
    "    r2s = []\n",
    "    MSEs = []\n",
    "    pca = PCA(n_components=.95)\n",
    "\n",
    "    for model in model_types:\n",
    "        pipe = make_pipeline(pca, model)\n",
    "        models.append(str(model)[:-2])\n",
    "        #pipe.fit(X_train_subset_scaled,Y_train)\n",
    "        #mod_pred = pipe.predict(X_validate_subset_scaled)\n",
    "        pipe.fit(dataset[0], Y_train)\n",
    "        mod_pred = pipe.predict(dataset[1])\n",
    "        r2s.append(r2_score(Y_validate, mod_pred))\n",
    "        MSEs.append(mean_squared_error(Y_validate, mod_pred))\n",
    "\n",
    "    tableData = [models,MSEs,r2s]\n",
    "    pcaRegResults = pd.DataFrame(tableData).T\n",
    "    pcaRegResults.columns=['model','MSE','r2']\n",
    "    return pcaRegResults.sort_values(by='r2',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e3a51aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled data set results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>MSE</th>\n",
       "      <th>r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LassoCV</td>\n",
       "      <td>155.162</td>\n",
       "      <td>0.60994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>156.421</td>\n",
       "      <td>0.606774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>156.532</td>\n",
       "      <td>0.606495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>159.86</td>\n",
       "      <td>0.598129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PoissonRegressor</td>\n",
       "      <td>160.682</td>\n",
       "      <td>0.596061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ElasticNet</td>\n",
       "      <td>167.171</td>\n",
       "      <td>0.57975</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              model      MSE        r2\n",
       "3           LassoCV  155.162   0.60994\n",
       "1             Ridge  156.421  0.606774\n",
       "0  LinearRegression  156.532  0.606495\n",
       "2             Lasso   159.86  0.598129\n",
       "5  PoissonRegressor  160.682  0.596061\n",
       "4        ElasticNet  167.171   0.57975"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Scaled data set results:')\n",
    "getPCAresults([X_train_scaled, X_validate_scaled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "806614db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed and scaled data set results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>MSE</th>\n",
       "      <th>r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PoissonRegressor</td>\n",
       "      <td>154.425</td>\n",
       "      <td>0.611791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LassoCV</td>\n",
       "      <td>155.659</td>\n",
       "      <td>0.608691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>156.249</td>\n",
       "      <td>0.607206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>156.56</td>\n",
       "      <td>0.606424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ElasticNet</td>\n",
       "      <td>167.687</td>\n",
       "      <td>0.578452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>169.441</td>\n",
       "      <td>0.574044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              model      MSE        r2\n",
       "5  PoissonRegressor  154.425  0.611791\n",
       "3           LassoCV  155.659  0.608691\n",
       "1             Ridge  156.249  0.607206\n",
       "0  LinearRegression   156.56  0.606424\n",
       "4        ElasticNet  167.687  0.578452\n",
       "2             Lasso  169.441  0.574044"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Transformed and scaled data set results:')\n",
    "getPCAresults([X_train_trans, X_validate_trans])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0f57709a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsetted, transformed, and scaled results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>MSE</th>\n",
       "      <th>r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PoissonRegressor</td>\n",
       "      <td>140.636</td>\n",
       "      <td>0.646456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>141.374</td>\n",
       "      <td>0.644601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>141.605</td>\n",
       "      <td>0.64402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LassoCV</td>\n",
       "      <td>144.497</td>\n",
       "      <td>0.63675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>155.099</td>\n",
       "      <td>0.610097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ElasticNet</td>\n",
       "      <td>159.098</td>\n",
       "      <td>0.600043</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              model      MSE        r2\n",
       "5  PoissonRegressor  140.636  0.646456\n",
       "1             Ridge  141.374  0.644601\n",
       "0  LinearRegression  141.605   0.64402\n",
       "3           LassoCV  144.497   0.63675\n",
       "2             Lasso  155.099  0.610097\n",
       "4        ElasticNet  159.098  0.600043"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Subsetted, transformed, and scaled results:')\n",
    "getPCAresults([X_train_subset_scaled, X_validate_subset_scaled])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc4433a",
   "metadata": {},
   "source": [
    "The transformed and the subset/transformed datasets are producing the best performing models.\n",
    "So I'll focus on those."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3218e6",
   "metadata": {},
   "source": [
    "XGBoost with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f2b06e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to find best parameters\n",
    "def getXGPCAparams(dataset):\n",
    "    pca = PCA(n_components=.95)\n",
    "    param_options = {'max_depth':[2,3,4],\n",
    "                    'learning_rate': [.15,.1,.09,.08],\n",
    "                    'gamma': [0,.2,.35,.5]}\n",
    "\n",
    "    #find best params using 90% of data on each iteration and 50% of cols per tree\n",
    "    find_bestPCA = GridSearchCV(\n",
    "        estimator = xg.XGBRegressor(seed = 43,\n",
    "                                   subsample=0.9,\n",
    "                                   colsample_bytree=0.5),\n",
    "        param_grid = param_options,\n",
    "        cv=3\n",
    "    )\n",
    "\n",
    "    pipe = make_pipeline(pca, find_bestPCA)\n",
    "    pipe.fit(dataset[0],Y_train)\n",
    "\n",
    "    #print(pipe.named_steps['gridsearchcv'].best_params_)\n",
    "    return [pipe.named_steps['gridsearchcv'].best_params_['gamma'],\n",
    "           pipe.named_steps['gridsearchcv'].best_params_['learning_rate'],\n",
    "           pipe.named_steps['gridsearchcv'].best_params_['max_depth']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "acba31f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to run XGBoost with PCA given dataset and parameters\n",
    "def getXgbModelPCA(dataset,gamma,learning_rate,max_depth):\n",
    "    xgbModelPCA = xg.XGBRegressor(seed = 43,\n",
    "                              gamma=gamma,\n",
    "                              learning_rate=learning_rate,\n",
    "                              max_depth=max_depth)\n",
    "\n",
    "    pipe = make_pipeline(pca, xgbModelPCA)\n",
    "    pipe.fit(dataset[0],Y_train)\n",
    "    pred=pipe.predict(dataset[1])\n",
    "    error = mean_squared_error(Y_validate, pred)\n",
    "    r2 = r2_score(Y_validate,pred)\n",
    "    return [error, r2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5acc5c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Transformed set\n",
      "error:  164.99816765040575 r2:  0.5852119675515963\n",
      "Results for Subset set\n",
      "error:  160.20537465327916 r2:  0.5972605448510893\n"
     ]
    }
   ],
   "source": [
    "#Run each of the datasets through the above function\n",
    "for dataset in [[X_train_trans, X_validate_trans, 'Transformed'],\n",
    "                [X_train_subset_scaled,X_validate_subset_scaled,'Subset']]:\n",
    "    print('Results for %s set'%dataset[2])\n",
    "    g, lr, md = getXGPCAparams(dataset)\n",
    "    error, r2 = getXgbModelPCA(dataset, g, lr, md)\n",
    "    print('error: ', error, 'r2: ', r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e660a74b",
   "metadata": {},
   "source": [
    "Random forest with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "dad1e75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to find best parameters\n",
    "def rvPCAparams(dataset):\n",
    "    pca = PCA(n_components=.95)\n",
    "    param_options = {'max_depth':[2,3,4,5,6],\n",
    "                    'min_samples_leaf': [2,3]\n",
    "                    }\n",
    "\n",
    "    #find best params using 90% of data on each iteration and 50% of cols per tree\n",
    "    find_bestPCA = GridSearchCV(\n",
    "        estimator = RandomForestRegressor(random_state=43),\n",
    "        param_grid = param_options,\n",
    "        cv=3\n",
    "    )\n",
    "\n",
    "    pipe = make_pipeline(pca, find_bestPCA)\n",
    "    pipe.fit(dataset[0],Y_train)\n",
    "\n",
    "    return [pipe.named_steps['gridsearchcv'].best_params_['max_depth'],\n",
    "            pipe.named_steps['gridsearchcv'].best_params_['min_samples_leaf']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81bfc1f7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#function to run a random forest model with PCA given a dataset and parameters\n",
    "def getRFpcaModel(dataset, md, msl):\n",
    "    regr_subset = RandomForestRegressor(max_depth=md, min_samples_leaf=msl, random_state=43)\n",
    "    pipe = make_pipeline(pca, regr_subset)\n",
    "    pipe.fit(dataset[0],Y_train)\n",
    "    regr_predict = pipe.predict(dataset[1])\n",
    "    error2 = mean_squared_error(Y_validate, regr_predict)\n",
    "    r2 = r2_score(Y_validate, regr_predict)\n",
    "    return [error2, r2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "2ec86bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Transformed set\n",
      "error:  168.3157722632574 r2:  0.5768718586315861\n",
      "Results for Subset set\n",
      "error:  123.80817675029209 r2:  0.6887592706842307\n"
     ]
    }
   ],
   "source": [
    "#Run the datasets through the above function\n",
    "for dataset in [[X_train_trans, X_validate_trans, 'Transformed'],\n",
    "                [X_train_subset_scaled,X_validate_subset_scaled,'Subset']]:\n",
    "    print('Results for %s set'%dataset[2])\n",
    "    md, msl = rvPCAparams(dataset)\n",
    "    error, r2 = getRFpcaModel(dataset, md, msl)\n",
    "    print('error: ', error, 'r2: ', r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "54e2b5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 2\n"
     ]
    }
   ],
   "source": [
    "#keep record of parameters for best model\n",
    "print(md, msl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a616547",
   "metadata": {},
   "source": [
    "The best model overall thus far is Random Forest with PCA using the subsetting dataset (which is also transformed).\n",
    "It yields an r-squared of ~0.69.  \n",
    "The second best model is the LassoCV on the transformed (not subsetted) dataset, with an r-squared of ~0.65."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "3134edad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get number of features in subsetted version\n",
    "len(X_train_subset_scaled.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "05e799c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get number of features in non-subsetted version\n",
    "len(X_train_trans.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bc602c",
   "metadata": {},
   "source": [
    "Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "c31e5865",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to find the best paramaters for a tensorflow model\n",
    "def buildModelTF(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Dense(hp.Choice('units1',[2,4,6]), input_dim=32, activation=\"relu\"))\n",
    "    for i in range(2):\n",
    "        model.add(layers.Dense(hp.Choice('units2',[50,150,200]),activation=\"relu\"))\n",
    "    model.add(layers.Dense(1))\n",
    "    keras.optimizers.Adam(learning_rate=hp.Choice('units3',[.0006,.00006,]))\n",
    "    model.compile(loss='mean_squared_error',optimizer='adam',metrics=['mse'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "88137971",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set early stopping condition\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "b1f042d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 01m 02s]\n",
      "val_mse: 238.29383087158203\n",
      "\n",
      "Best val_mse So Far: 129.1446647644043\n",
      "Total elapsed time: 00h 10m 48s\n",
      "\n",
      "Search: Running Trial #11\n",
      "\n",
      "Hyperparameter    |Value             |Best Value So Far \n",
      "units1            |6                 |2                 \n",
      "units2            |50                |50                \n",
      "units3            |6e-05             |6e-05             \n",
      "\n",
      "Epoch 1/350\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 16050.8418 - mse: 16050.8418 - val_loss: 16045.7705 - val_mse: 16045.7705\n",
      "Epoch 2/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 15949.0781 - mse: 15949.0781 - val_loss: 15938.3242 - val_mse: 15938.3242\n",
      "Epoch 3/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 15838.9268 - mse: 15838.9268 - val_loss: 15797.0000 - val_mse: 15796.9971\n",
      "Epoch 4/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 15680.5801 - mse: 15680.5801 - val_loss: 15592.4990 - val_mse: 15592.4990\n",
      "Epoch 5/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 15447.7354 - mse: 15447.7354 - val_loss: 15293.1064 - val_mse: 15293.1064\n",
      "Epoch 6/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 15114.7666 - mse: 15114.7666 - val_loss: 14858.0283 - val_mse: 14858.0283\n",
      "Epoch 7/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 14601.9844 - mse: 14601.9844 - val_loss: 14233.7266 - val_mse: 14233.7266\n",
      "Epoch 8/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 13872.3115 - mse: 13872.3115 - val_loss: 13344.4971 - val_mse: 13344.4971\n",
      "Epoch 9/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 12825.7881 - mse: 12825.7881 - val_loss: 12138.3740 - val_mse: 12138.3740\n",
      "Epoch 10/350\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 11448.6699 - mse: 11448.6699 - val_loss: 10583.0742 - val_mse: 10583.0742\n",
      "Epoch 11/350\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 9653.7305 - mse: 9653.7305 - val_loss: 8756.7900 - val_mse: 8756.7900\n",
      "Epoch 12/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 7660.5171 - mse: 7660.5171 - val_loss: 6827.7202 - val_mse: 6827.7202\n",
      "Epoch 13/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 5684.0825 - mse: 5684.0825 - val_loss: 5160.6968 - val_mse: 5160.6968\n",
      "Epoch 14/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 4038.8425 - mse: 4038.8425 - val_loss: 4026.9600 - val_mse: 4026.9600\n",
      "Epoch 15/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 3073.4050 - mse: 3073.4050 - val_loss: 3384.1230 - val_mse: 3384.1230\n",
      "Epoch 16/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 2555.4253 - mse: 2555.4253 - val_loss: 3036.5618 - val_mse: 3036.5618\n",
      "Epoch 17/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 2271.3113 - mse: 2271.3113 - val_loss: 2734.8586 - val_mse: 2734.8586\n",
      "Epoch 18/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 2029.2454 - mse: 2029.2454 - val_loss: 2507.7070 - val_mse: 2507.7070\n",
      "Epoch 19/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 1862.6884 - mse: 1862.6884 - val_loss: 2337.9021 - val_mse: 2337.9021\n",
      "Epoch 20/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1748.3210 - mse: 1748.3210 - val_loss: 2202.8066 - val_mse: 2202.8066\n",
      "Epoch 21/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 1654.6691 - mse: 1654.6691 - val_loss: 2093.0439 - val_mse: 2093.0439\n",
      "Epoch 22/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1580.4255 - mse: 1580.4255 - val_loss: 2000.5092 - val_mse: 2000.5092\n",
      "Epoch 23/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1510.2618 - mse: 1510.2618 - val_loss: 1924.4391 - val_mse: 1924.4391\n",
      "Epoch 24/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 1436.6278 - mse: 1436.6278 - val_loss: 1864.1113 - val_mse: 1864.1113\n",
      "Epoch 25/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1378.2230 - mse: 1378.2230 - val_loss: 1799.7716 - val_mse: 1799.7716\n",
      "Epoch 26/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 1313.0260 - mse: 1313.0260 - val_loss: 1741.1451 - val_mse: 1741.1451\n",
      "Epoch 27/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1256.8286 - mse: 1256.8286 - val_loss: 1677.1512 - val_mse: 1677.1512\n",
      "Epoch 28/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1202.1398 - mse: 1202.1398 - val_loss: 1620.1349 - val_mse: 1620.1349\n",
      "Epoch 29/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1152.0728 - mse: 1152.0728 - val_loss: 1574.6343 - val_mse: 1574.6343\n",
      "Epoch 30/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 1105.4318 - mse: 1105.4318 - val_loss: 1522.9745 - val_mse: 1522.9745\n",
      "Epoch 31/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 1058.4746 - mse: 1058.4746 - val_loss: 1470.5568 - val_mse: 1470.5568\n",
      "Epoch 32/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 1014.8656 - mse: 1014.8656 - val_loss: 1424.2264 - val_mse: 1424.2264\n",
      "Epoch 33/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 979.9238 - mse: 979.9238 - val_loss: 1387.1060 - val_mse: 1387.1060\n",
      "Epoch 34/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 937.1293 - mse: 937.1293 - val_loss: 1341.7496 - val_mse: 1341.7496\n",
      "Epoch 35/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 902.2462 - mse: 902.2462 - val_loss: 1296.5305 - val_mse: 1296.5305\n",
      "Epoch 36/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 862.4462 - mse: 862.4462 - val_loss: 1255.3732 - val_mse: 1255.3732\n",
      "Epoch 37/350\n",
      "10/10 [==============================] - ETA: 0s - loss: 1114.3148 - mse: 1114.314 - 0s 4ms/step - loss: 831.2402 - mse: 831.2402 - val_loss: 1228.3127 - val_mse: 1228.3127\n",
      "Epoch 38/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 796.6436 - mse: 796.6436 - val_loss: 1183.4559 - val_mse: 1183.4559\n",
      "Epoch 39/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 770.4623 - mse: 770.4623 - val_loss: 1147.8453 - val_mse: 1147.8453\n",
      "Epoch 40/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 738.3073 - mse: 738.3073 - val_loss: 1113.9082 - val_mse: 1113.9082\n",
      "Epoch 41/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 713.3491 - mse: 713.3491 - val_loss: 1079.3210 - val_mse: 1079.3210\n",
      "Epoch 42/350\n",
      "10/10 [==============================] - ETA: 0s - loss: 464.6475 - mse: 464.647 - 0s 4ms/step - loss: 687.5425 - mse: 687.5425 - val_loss: 1048.3704 - val_mse: 1048.3704\n",
      "Epoch 43/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 665.5562 - mse: 665.5562 - val_loss: 1020.8342 - val_mse: 1020.8342\n",
      "Epoch 44/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 642.5450 - mse: 642.5450 - val_loss: 992.5115 - val_mse: 992.5115\n",
      "Epoch 45/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 623.5717 - mse: 623.5717 - val_loss: 964.3848 - val_mse: 964.3848\n",
      "Epoch 46/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 601.7017 - mse: 601.7017 - val_loss: 931.8527 - val_mse: 931.8527\n",
      "Epoch 47/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 581.4282 - mse: 581.4282 - val_loss: 905.2879 - val_mse: 905.2879\n",
      "Epoch 48/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 562.2983 - mse: 562.2983 - val_loss: 880.3333 - val_mse: 880.3333\n",
      "Epoch 49/350\n",
      "10/10 [==============================] - ETA: 0s - loss: 241.1294 - mse: 241.129 - 0s 4ms/step - loss: 546.0410 - mse: 546.0410 - val_loss: 857.2003 - val_mse: 857.2004\n",
      "Epoch 50/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 531.4725 - mse: 531.4725 - val_loss: 832.4345 - val_mse: 832.4345\n",
      "Epoch 51/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 514.8075 - mse: 514.8075 - val_loss: 814.3533 - val_mse: 814.3533\n",
      "Epoch 52/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 500.4767 - mse: 500.4767 - val_loss: 790.0552 - val_mse: 790.0552\n",
      "Epoch 53/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 486.2468 - mse: 486.2468 - val_loss: 764.9807 - val_mse: 764.9807\n",
      "Epoch 54/350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 4ms/step - loss: 472.0287 - mse: 472.0287 - val_loss: 744.1243 - val_mse: 744.1243\n",
      "Epoch 55/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 458.9284 - mse: 458.9284 - val_loss: 724.9468 - val_mse: 724.9468\n",
      "Epoch 56/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 449.0683 - mse: 449.0683 - val_loss: 708.7001 - val_mse: 708.7001\n",
      "Epoch 57/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 437.6796 - mse: 437.6796 - val_loss: 696.6753 - val_mse: 696.6753\n",
      "Epoch 58/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 426.1929 - mse: 426.1929 - val_loss: 671.7593 - val_mse: 671.7593\n",
      "Epoch 59/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 415.1340 - mse: 415.1340 - val_loss: 656.2871 - val_mse: 656.2871\n",
      "Epoch 60/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 403.2608 - mse: 403.2608 - val_loss: 639.8084 - val_mse: 639.8084\n",
      "Epoch 61/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 394.4105 - mse: 394.4105 - val_loss: 624.0938 - val_mse: 624.0938\n",
      "Epoch 62/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 385.0022 - mse: 385.0022 - val_loss: 613.7372 - val_mse: 613.7372\n",
      "Epoch 63/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 376.2071 - mse: 376.2071 - val_loss: 601.9703 - val_mse: 601.9703\n",
      "Epoch 64/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 367.7160 - mse: 367.7160 - val_loss: 588.3391 - val_mse: 588.3391\n",
      "Epoch 65/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 358.3035 - mse: 358.3035 - val_loss: 576.6094 - val_mse: 576.6094\n",
      "Epoch 66/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 351.3331 - mse: 351.3331 - val_loss: 567.0706 - val_mse: 567.0706\n",
      "Epoch 67/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 343.3339 - mse: 343.3339 - val_loss: 556.4943 - val_mse: 556.4943\n",
      "Epoch 68/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 335.9259 - mse: 335.9259 - val_loss: 548.6827 - val_mse: 548.6826\n",
      "Epoch 69/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 329.3101 - mse: 329.3101 - val_loss: 540.2866 - val_mse: 540.2866\n",
      "Epoch 70/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 323.1375 - mse: 323.1375 - val_loss: 531.1800 - val_mse: 531.1800\n",
      "Epoch 71/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 318.6333 - mse: 318.6333 - val_loss: 523.6413 - val_mse: 523.6413\n",
      "Epoch 72/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 310.3795 - mse: 310.3795 - val_loss: 512.0799 - val_mse: 512.0799\n",
      "Epoch 73/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 305.7278 - mse: 305.7278 - val_loss: 504.9123 - val_mse: 504.9123\n",
      "Epoch 74/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 298.2976 - mse: 298.2976 - val_loss: 494.1855 - val_mse: 494.1855\n",
      "Epoch 75/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 292.7234 - mse: 292.7234 - val_loss: 486.9969 - val_mse: 486.9969\n",
      "Epoch 76/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 286.6132 - mse: 286.6132 - val_loss: 478.8742 - val_mse: 478.8742\n",
      "Epoch 77/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 281.9005 - mse: 281.9005 - val_loss: 469.3091 - val_mse: 469.3091\n",
      "Epoch 78/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 276.9682 - mse: 276.9682 - val_loss: 462.3833 - val_mse: 462.3833\n",
      "Epoch 79/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 271.7036 - mse: 271.7036 - val_loss: 455.9818 - val_mse: 455.9818\n",
      "Epoch 80/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 267.5609 - mse: 267.5609 - val_loss: 448.8182 - val_mse: 448.8182\n",
      "Epoch 81/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 261.7720 - mse: 261.7720 - val_loss: 443.6571 - val_mse: 443.6571\n",
      "Epoch 82/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 258.7267 - mse: 258.7267 - val_loss: 436.4456 - val_mse: 436.4456\n",
      "Epoch 83/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 253.5429 - mse: 253.5429 - val_loss: 426.9658 - val_mse: 426.9657\n",
      "Epoch 84/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 249.7235 - mse: 249.7235 - val_loss: 420.9827 - val_mse: 420.9828\n",
      "Epoch 85/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 244.8537 - mse: 244.8537 - val_loss: 414.4214 - val_mse: 414.4214\n",
      "Epoch 86/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 241.5535 - mse: 241.5535 - val_loss: 409.3324 - val_mse: 409.3324\n",
      "Epoch 87/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 236.7399 - mse: 236.7399 - val_loss: 404.2139 - val_mse: 404.2139\n",
      "Epoch 88/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 232.7292 - mse: 232.7292 - val_loss: 397.1990 - val_mse: 397.1990\n",
      "Epoch 89/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 230.1778 - mse: 230.1777 - val_loss: 391.3948 - val_mse: 391.3948\n",
      "Epoch 90/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 226.1358 - mse: 226.1358 - val_loss: 385.5108 - val_mse: 385.5108\n",
      "Epoch 91/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 222.8478 - mse: 222.8478 - val_loss: 382.2797 - val_mse: 382.2797\n",
      "Epoch 92/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 219.6284 - mse: 219.6284 - val_loss: 377.6173 - val_mse: 377.6173\n",
      "Epoch 93/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 216.7903 - mse: 216.7903 - val_loss: 368.9656 - val_mse: 368.9656\n",
      "Epoch 94/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 214.2680 - mse: 214.2680 - val_loss: 365.5786 - val_mse: 365.5786\n",
      "Epoch 95/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 210.0381 - mse: 210.0381 - val_loss: 361.1208 - val_mse: 361.1208\n",
      "Epoch 96/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 207.4585 - mse: 207.4585 - val_loss: 358.3744 - val_mse: 358.3744\n",
      "Epoch 97/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 203.6058 - mse: 203.6058 - val_loss: 353.4395 - val_mse: 353.4395\n",
      "Epoch 98/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 201.9833 - mse: 201.9833 - val_loss: 347.6798 - val_mse: 347.6798\n",
      "Epoch 99/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 198.8009 - mse: 198.8009 - val_loss: 344.2481 - val_mse: 344.2481\n",
      "Epoch 100/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 196.5612 - mse: 196.5612 - val_loss: 341.1586 - val_mse: 341.1587\n",
      "Epoch 101/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 193.9924 - mse: 193.9924 - val_loss: 339.9779 - val_mse: 339.9779\n",
      "Epoch 102/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 191.3468 - mse: 191.3468 - val_loss: 334.7461 - val_mse: 334.7461\n",
      "Epoch 103/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 189.3479 - mse: 189.3479 - val_loss: 328.9315 - val_mse: 328.9315\n",
      "Epoch 104/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 188.9012 - mse: 188.9012 - val_loss: 326.6187 - val_mse: 326.6187\n",
      "Epoch 105/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 185.1155 - mse: 185.1155 - val_loss: 324.1728 - val_mse: 324.1728\n",
      "Epoch 106/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 182.1164 - mse: 182.1164 - val_loss: 317.3208 - val_mse: 317.3208\n",
      "Epoch 107/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 180.3513 - mse: 180.3513 - val_loss: 314.8142 - val_mse: 314.8142\n",
      "Epoch 108/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 177.6210 - mse: 177.6210 - val_loss: 313.3304 - val_mse: 313.3304\n",
      "Epoch 109/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 176.5075 - mse: 176.5075 - val_loss: 310.6110 - val_mse: 310.6110\n",
      "Epoch 110/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 174.4584 - mse: 174.4584 - val_loss: 308.4751 - val_mse: 308.4751\n",
      "Epoch 111/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 172.2674 - mse: 172.2674 - val_loss: 302.3686 - val_mse: 302.3686\n",
      "Epoch 112/350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 4ms/step - loss: 169.6520 - mse: 169.6520 - val_loss: 300.5250 - val_mse: 300.5250\n",
      "Epoch 113/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 168.8745 - mse: 168.8745 - val_loss: 300.2885 - val_mse: 300.2885\n",
      "Epoch 114/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 165.8292 - mse: 165.8292 - val_loss: 295.2478 - val_mse: 295.2478\n",
      "Epoch 115/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 164.6354 - mse: 164.6354 - val_loss: 292.6971 - val_mse: 292.6971\n",
      "Epoch 116/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 163.0782 - mse: 163.0782 - val_loss: 291.5854 - val_mse: 291.5854\n",
      "Epoch 117/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 161.3363 - mse: 161.3363 - val_loss: 289.0187 - val_mse: 289.0187\n",
      "Epoch 118/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 159.6923 - mse: 159.6923 - val_loss: 283.9474 - val_mse: 283.9474\n",
      "Epoch 119/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 157.4259 - mse: 157.4259 - val_loss: 281.6717 - val_mse: 281.6717\n",
      "Epoch 120/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 157.7428 - mse: 157.7428 - val_loss: 279.2110 - val_mse: 279.2110\n",
      "Epoch 121/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 154.7510 - mse: 154.7510 - val_loss: 278.2751 - val_mse: 278.2751\n",
      "Epoch 122/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 154.2855 - mse: 154.2855 - val_loss: 273.3592 - val_mse: 273.3592\n",
      "Epoch 123/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 152.7779 - mse: 152.7779 - val_loss: 274.3190 - val_mse: 274.3190\n",
      "Epoch 124/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 150.7238 - mse: 150.7238 - val_loss: 268.3047 - val_mse: 268.3047\n",
      "Epoch 125/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 148.8781 - mse: 148.8781 - val_loss: 266.2487 - val_mse: 266.2487\n",
      "Epoch 126/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 147.1770 - mse: 147.1770 - val_loss: 265.6631 - val_mse: 265.6631\n",
      "Epoch 127/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 146.7964 - mse: 146.7964 - val_loss: 264.5834 - val_mse: 264.5834\n",
      "Epoch 128/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 147.5483 - mse: 147.5483 - val_loss: 258.9203 - val_mse: 258.9203\n",
      "Epoch 129/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 144.2896 - mse: 144.2896 - val_loss: 259.9968 - val_mse: 259.9968\n",
      "Epoch 130/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 143.1037 - mse: 143.1037 - val_loss: 255.3871 - val_mse: 255.3871\n",
      "Epoch 131/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 140.5206 - mse: 140.5206 - val_loss: 254.7901 - val_mse: 254.7901\n",
      "Epoch 132/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 139.9753 - mse: 139.9753 - val_loss: 254.9171 - val_mse: 254.9172\n",
      "Epoch 133/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 138.7877 - mse: 138.7877 - val_loss: 251.7546 - val_mse: 251.7546\n",
      "Epoch 134/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 140.5141 - mse: 140.5141 - val_loss: 248.3798 - val_mse: 248.3798\n",
      "Epoch 135/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 136.2841 - mse: 136.2841 - val_loss: 247.2184 - val_mse: 247.2184\n",
      "Epoch 136/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 135.4357 - mse: 135.4357 - val_loss: 248.3425 - val_mse: 248.3425\n",
      "Epoch 137/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 134.8333 - mse: 134.8333 - val_loss: 246.1865 - val_mse: 246.1865\n",
      "Epoch 138/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 134.6974 - mse: 134.6974 - val_loss: 241.3433 - val_mse: 241.3433\n",
      "Epoch 139/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 131.8899 - mse: 131.8899 - val_loss: 240.8140 - val_mse: 240.8140\n",
      "Epoch 140/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 134.0995 - mse: 134.0995 - val_loss: 243.7396 - val_mse: 243.7396\n",
      "Epoch 141/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 133.0563 - mse: 133.0563 - val_loss: 235.8915 - val_mse: 235.8915\n",
      "Epoch 142/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 129.8144 - mse: 129.8144 - val_loss: 235.5549 - val_mse: 235.5549\n",
      "Epoch 143/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 128.7529 - mse: 128.7529 - val_loss: 233.8201 - val_mse: 233.8201\n",
      "Epoch 144/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 128.2426 - mse: 128.2426 - val_loss: 233.7157 - val_mse: 233.7157\n",
      "Epoch 145/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 126.8816 - mse: 126.8816 - val_loss: 231.3990 - val_mse: 231.3990\n",
      "Epoch 146/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 125.6703 - mse: 125.6703 - val_loss: 229.7985 - val_mse: 229.7985\n",
      "Epoch 147/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 124.4649 - mse: 124.4649 - val_loss: 227.3339 - val_mse: 227.3339\n",
      "Epoch 148/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 125.0860 - mse: 125.0860 - val_loss: 225.8672 - val_mse: 225.8672\n",
      "Epoch 149/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 123.2894 - mse: 123.2894 - val_loss: 225.9499 - val_mse: 225.9499\n",
      "Epoch 150/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 122.6687 - mse: 122.6687 - val_loss: 224.5933 - val_mse: 224.5933\n",
      "Epoch 151/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 121.4521 - mse: 121.4521 - val_loss: 222.7918 - val_mse: 222.7918\n",
      "Epoch 152/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 121.2013 - mse: 121.2013 - val_loss: 220.7457 - val_mse: 220.7457\n",
      "Epoch 153/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 120.8722 - mse: 120.8722 - val_loss: 220.3578 - val_mse: 220.3578\n",
      "Epoch 154/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 120.5729 - mse: 120.5729 - val_loss: 216.9967 - val_mse: 216.9967\n",
      "Epoch 155/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 118.5448 - mse: 118.5448 - val_loss: 217.8082 - val_mse: 217.8082\n",
      "Epoch 156/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 118.2293 - mse: 118.2293 - val_loss: 215.9078 - val_mse: 215.9078\n",
      "Epoch 157/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 117.8026 - mse: 117.8026 - val_loss: 214.5248 - val_mse: 214.5248\n",
      "Epoch 158/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 117.2521 - mse: 117.2521 - val_loss: 215.2410 - val_mse: 215.2410\n",
      "Epoch 159/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 116.3701 - mse: 116.3701 - val_loss: 212.6652 - val_mse: 212.6652\n",
      "Epoch 160/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 115.9217 - mse: 115.9217 - val_loss: 212.6009 - val_mse: 212.6009\n",
      "Epoch 161/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 114.8300 - mse: 114.8300 - val_loss: 210.2087 - val_mse: 210.2087\n",
      "Epoch 162/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 113.7692 - mse: 113.7692 - val_loss: 209.1643 - val_mse: 209.1643\n",
      "Epoch 163/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 113.8404 - mse: 113.8404 - val_loss: 208.1592 - val_mse: 208.1592\n",
      "Epoch 164/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 114.4672 - mse: 114.4672 - val_loss: 205.2696 - val_mse: 205.2696\n",
      "Epoch 165/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 112.0404 - mse: 112.0404 - val_loss: 206.1237 - val_mse: 206.1238\n",
      "Epoch 166/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 112.6188 - mse: 112.6188 - val_loss: 204.3681 - val_mse: 204.3681\n",
      "Epoch 167/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 111.8482 - mse: 111.8482 - val_loss: 203.6099 - val_mse: 203.6099\n",
      "Epoch 168/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 110.4565 - mse: 110.4565 - val_loss: 204.0329 - val_mse: 204.0329\n",
      "Epoch 169/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 111.1304 - mse: 111.1304 - val_loss: 201.2614 - val_mse: 201.2614\n",
      "Epoch 170/350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 4ms/step - loss: 110.1328 - mse: 110.1328 - val_loss: 201.8722 - val_mse: 201.8722\n",
      "Epoch 171/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 108.2668 - mse: 108.2668 - val_loss: 199.9619 - val_mse: 199.9619\n",
      "Epoch 172/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 108.4329 - mse: 108.4329 - val_loss: 198.6420 - val_mse: 198.6420\n",
      "Epoch 173/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 108.1377 - mse: 108.1377 - val_loss: 198.5297 - val_mse: 198.5297\n",
      "Epoch 174/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 108.3929 - mse: 108.3929 - val_loss: 196.0857 - val_mse: 196.0857\n",
      "Epoch 175/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 107.9714 - mse: 107.9714 - val_loss: 199.4732 - val_mse: 199.4731\n",
      "Epoch 176/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 105.4060 - mse: 105.4060 - val_loss: 194.8321 - val_mse: 194.8321\n",
      "Epoch 177/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 106.3099 - mse: 106.3099 - val_loss: 194.2741 - val_mse: 194.2741\n",
      "Epoch 178/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 105.3432 - mse: 105.3432 - val_loss: 195.0437 - val_mse: 195.0437\n",
      "Epoch 179/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 105.6833 - mse: 105.6833 - val_loss: 194.5518 - val_mse: 194.5518\n",
      "Epoch 180/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 104.3822 - mse: 104.3822 - val_loss: 192.2809 - val_mse: 192.2809\n",
      "Epoch 181/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 104.2079 - mse: 104.2079 - val_loss: 192.4173 - val_mse: 192.4173\n",
      "Epoch 182/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 103.4572 - mse: 103.4572 - val_loss: 193.3257 - val_mse: 193.3257\n",
      "Epoch 183/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 103.2665 - mse: 103.2665 - val_loss: 191.0647 - val_mse: 191.0647\n",
      "Epoch 184/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 105.0079 - mse: 105.0079 - val_loss: 188.7761 - val_mse: 188.7761\n",
      "Epoch 185/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 103.0427 - mse: 103.0427 - val_loss: 190.9623 - val_mse: 190.9623\n",
      "Epoch 186/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 101.7223 - mse: 101.7223 - val_loss: 187.9033 - val_mse: 187.9033\n",
      "Epoch 187/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 102.3514 - mse: 102.3514 - val_loss: 187.2846 - val_mse: 187.2846\n",
      "Epoch 188/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 102.1082 - mse: 102.1082 - val_loss: 190.0735 - val_mse: 190.0735\n",
      "Epoch 189/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 100.7623 - mse: 100.7623 - val_loss: 185.4546 - val_mse: 185.4546\n",
      "Epoch 190/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 100.6986 - mse: 100.6986 - val_loss: 185.3347 - val_mse: 185.3347\n",
      "Epoch 191/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 99.6779 - mse: 99.6779 - val_loss: 187.0989 - val_mse: 187.0989\n",
      "Epoch 192/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 99.6330 - mse: 99.6330 - val_loss: 184.6814 - val_mse: 184.6814\n",
      "Epoch 193/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 99.1953 - mse: 99.1953 - val_loss: 183.3395 - val_mse: 183.3395\n",
      "Epoch 194/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 99.3520 - mse: 99.3520 - val_loss: 183.1983 - val_mse: 183.1983\n",
      "Epoch 195/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 97.9202 - mse: 97.9202 - val_loss: 184.8255 - val_mse: 184.8255\n",
      "Epoch 196/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 97.9083 - mse: 97.9083 - val_loss: 182.8901 - val_mse: 182.8901\n",
      "Epoch 197/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 97.8735 - mse: 97.8735 - val_loss: 179.9927 - val_mse: 179.9927\n",
      "Epoch 198/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 97.1986 - mse: 97.1986 - val_loss: 181.4082 - val_mse: 181.4082\n",
      "Epoch 199/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 97.1944 - mse: 97.1944 - val_loss: 182.1106 - val_mse: 182.1106\n",
      "Epoch 200/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 98.3146 - mse: 98.3146 - val_loss: 182.6347 - val_mse: 182.6347\n",
      "Epoch 201/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 98.8970 - mse: 98.8970 - val_loss: 178.7797 - val_mse: 178.7797\n",
      "Epoch 202/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 96.7337 - mse: 96.7337 - val_loss: 182.5955 - val_mse: 182.5955\n",
      "Epoch 203/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 95.9050 - mse: 95.9050 - val_loss: 179.4958 - val_mse: 179.4958\n",
      "Epoch 204/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 95.8755 - mse: 95.8755 - val_loss: 178.6134 - val_mse: 178.6134\n",
      "Epoch 205/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 94.7859 - mse: 94.7859 - val_loss: 179.6048 - val_mse: 179.6048\n",
      "Epoch 206/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 95.1069 - mse: 95.1069 - val_loss: 178.6282 - val_mse: 178.6282\n",
      "Epoch 207/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 94.3424 - mse: 94.3424 - val_loss: 180.1437 - val_mse: 180.1437\n",
      "Epoch 208/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 94.2353 - mse: 94.2353 - val_loss: 178.1251 - val_mse: 178.1251\n",
      "Epoch 209/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 93.7508 - mse: 93.7508 - val_loss: 178.5893 - val_mse: 178.5893\n",
      "Epoch 210/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 93.4315 - mse: 93.4315 - val_loss: 177.3779 - val_mse: 177.3779\n",
      "Epoch 211/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 93.0391 - mse: 93.0391 - val_loss: 176.6819 - val_mse: 176.6819\n",
      "Epoch 212/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 93.1678 - mse: 93.1678 - val_loss: 175.2288 - val_mse: 175.2288\n",
      "Epoch 213/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 92.4618 - mse: 92.4618 - val_loss: 179.0731 - val_mse: 179.0731\n",
      "Epoch 214/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 92.5657 - mse: 92.5657 - val_loss: 175.9722 - val_mse: 175.9722\n",
      "Epoch 215/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 91.7777 - mse: 91.7777 - val_loss: 176.1257 - val_mse: 176.1257\n",
      "Epoch 216/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 90.9436 - mse: 90.9436 - val_loss: 175.6854 - val_mse: 175.6854\n",
      "Epoch 217/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 92.7735 - mse: 92.7735 - val_loss: 176.0092 - val_mse: 176.0092\n",
      "Epoch 218/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 91.6445 - mse: 91.6445 - val_loss: 173.4536 - val_mse: 173.4536\n",
      "Epoch 219/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 90.6078 - mse: 90.6078 - val_loss: 175.9413 - val_mse: 175.9413\n",
      "Epoch 220/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 91.9775 - mse: 91.9775 - val_loss: 178.7739 - val_mse: 178.7739\n",
      "Epoch 221/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 89.4905 - mse: 89.4905 - val_loss: 172.8035 - val_mse: 172.8035\n",
      "Epoch 222/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 90.7335 - mse: 90.7335 - val_loss: 172.2410 - val_mse: 172.2410\n",
      "Epoch 223/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 89.4871 - mse: 89.4871 - val_loss: 175.2471 - val_mse: 175.2471\n",
      "Epoch 224/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 89.6162 - mse: 89.6162 - val_loss: 173.4698 - val_mse: 173.4698\n",
      "Epoch 225/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 89.1267 - mse: 89.1267 - val_loss: 172.1971 - val_mse: 172.1971\n",
      "Epoch 226/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 88.8676 - mse: 88.8676 - val_loss: 173.9965 - val_mse: 173.9965\n",
      "Epoch 227/350\n",
      "10/10 [==============================] - ETA: 0s - loss: 68.4001 - mse: 68.400 - 0s 4ms/step - loss: 89.0552 - mse: 89.0552 - val_loss: 175.1418 - val_mse: 175.1417\n",
      "Epoch 228/350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 3ms/step - loss: 89.1579 - mse: 89.1579 - val_loss: 172.3236 - val_mse: 172.3236\n",
      "Epoch 229/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 89.0707 - mse: 89.0707 - val_loss: 170.4421 - val_mse: 170.4421\n",
      "Epoch 230/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 88.2877 - mse: 88.2877 - val_loss: 171.1669 - val_mse: 171.1669\n",
      "Epoch 231/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 87.9603 - mse: 87.9603 - val_loss: 173.9234 - val_mse: 173.9234\n",
      "Epoch 232/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 87.4901 - mse: 87.4901 - val_loss: 172.0799 - val_mse: 172.0799\n",
      "Epoch 233/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 86.8729 - mse: 86.8729 - val_loss: 169.9355 - val_mse: 169.9356\n",
      "Epoch 234/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 86.8548 - mse: 86.8548 - val_loss: 170.9018 - val_mse: 170.9018\n",
      "Epoch 235/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 86.9579 - mse: 86.9579 - val_loss: 170.5427 - val_mse: 170.5427\n",
      "Epoch 236/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 86.3557 - mse: 86.3557 - val_loss: 169.4077 - val_mse: 169.4077\n",
      "Epoch 237/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 86.0321 - mse: 86.0321 - val_loss: 170.1494 - val_mse: 170.1494\n",
      "Epoch 238/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 85.8123 - mse: 85.8123 - val_loss: 170.0868 - val_mse: 170.0868\n",
      "Epoch 239/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 85.8962 - mse: 85.8962 - val_loss: 169.2019 - val_mse: 169.2019\n",
      "Epoch 240/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 85.1150 - mse: 85.1150 - val_loss: 170.0140 - val_mse: 170.0140\n",
      "Epoch 241/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 85.0084 - mse: 85.0084 - val_loss: 171.2692 - val_mse: 171.2692\n",
      "Epoch 242/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 85.1247 - mse: 85.1247 - val_loss: 169.6444 - val_mse: 169.6444\n",
      "Epoch 243/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 85.0253 - mse: 85.0253 - val_loss: 168.6377 - val_mse: 168.6377\n",
      "Epoch 244/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 84.9051 - mse: 84.9051 - val_loss: 170.9544 - val_mse: 170.9544\n",
      "Epoch 245/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 84.7713 - mse: 84.7713 - val_loss: 169.3031 - val_mse: 169.3031\n",
      "Epoch 246/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 84.5983 - mse: 84.5983 - val_loss: 171.0821 - val_mse: 171.0821\n",
      "Epoch 247/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 83.5764 - mse: 83.5764 - val_loss: 168.4730 - val_mse: 168.4730\n",
      "Epoch 248/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 83.5663 - mse: 83.5663 - val_loss: 168.2641 - val_mse: 168.2641\n",
      "Epoch 249/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 83.3013 - mse: 83.3013 - val_loss: 168.9791 - val_mse: 168.9791\n",
      "Epoch 250/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 84.2251 - mse: 84.2251 - val_loss: 169.2283 - val_mse: 169.2283\n",
      "Epoch 251/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 83.9118 - mse: 83.9118 - val_loss: 166.8115 - val_mse: 166.8116\n",
      "Epoch 252/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 82.3304 - mse: 82.3304 - val_loss: 168.0369 - val_mse: 168.0369\n",
      "Epoch 253/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 82.6447 - mse: 82.6447 - val_loss: 168.6245 - val_mse: 168.6245\n",
      "Epoch 254/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 82.0223 - mse: 82.0223 - val_loss: 167.4470 - val_mse: 167.4470\n",
      "Epoch 255/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 83.6448 - mse: 83.6448 - val_loss: 166.6446 - val_mse: 166.6446\n",
      "Epoch 256/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 81.3981 - mse: 81.3981 - val_loss: 170.5690 - val_mse: 170.5690\n",
      "Epoch 257/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 82.7467 - mse: 82.7467 - val_loss: 168.1411 - val_mse: 168.1411\n",
      "Epoch 258/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 81.4445 - mse: 81.4445 - val_loss: 167.8882 - val_mse: 167.8882\n",
      "Epoch 259/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 81.3577 - mse: 81.3577 - val_loss: 168.4728 - val_mse: 168.4728\n",
      "Epoch 260/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 81.7436 - mse: 81.7436 - val_loss: 166.2116 - val_mse: 166.2116\n",
      "Epoch 261/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 81.0515 - mse: 81.0515 - val_loss: 170.1621 - val_mse: 170.1621\n",
      "Epoch 262/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 81.1235 - mse: 81.1235 - val_loss: 165.8802 - val_mse: 165.8802\n",
      "Epoch 263/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 80.3493 - mse: 80.3493 - val_loss: 168.1382 - val_mse: 168.1382\n",
      "Epoch 264/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 80.9529 - mse: 80.9529 - val_loss: 169.4924 - val_mse: 169.4924\n",
      "Epoch 265/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 80.1869 - mse: 80.1869 - val_loss: 165.7388 - val_mse: 165.7388\n",
      "Epoch 266/350\n",
      "10/10 [==============================] - ETA: 0s - loss: 100.4535 - mse: 100.453 - 0s 3ms/step - loss: 79.9814 - mse: 79.9814 - val_loss: 166.3074 - val_mse: 166.3074\n",
      "Epoch 267/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 79.8585 - mse: 79.8585 - val_loss: 167.0738 - val_mse: 167.0738\n",
      "Epoch 268/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 79.1962 - mse: 79.1962 - val_loss: 166.3880 - val_mse: 166.3880\n",
      "Epoch 269/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 80.0614 - mse: 80.0614 - val_loss: 165.3055 - val_mse: 165.3055\n",
      "Epoch 270/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 79.3250 - mse: 79.3250 - val_loss: 168.1736 - val_mse: 168.1736\n",
      "Epoch 271/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 80.0340 - mse: 80.0340 - val_loss: 164.9417 - val_mse: 164.9417\n",
      "Epoch 272/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 78.9238 - mse: 78.9238 - val_loss: 167.5587 - val_mse: 167.5587\n",
      "Epoch 273/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 78.3792 - mse: 78.3792 - val_loss: 165.7153 - val_mse: 165.7153\n",
      "Epoch 274/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 78.1510 - mse: 78.1510 - val_loss: 166.2224 - val_mse: 166.2224\n",
      "Epoch 275/350\n",
      "10/10 [==============================] - ETA: 0s - loss: 42.4094 - mse: 42.409 - 0s 3ms/step - loss: 78.7585 - mse: 78.7585 - val_loss: 165.2375 - val_mse: 165.2375\n",
      "Epoch 276/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 78.2993 - mse: 78.2993 - val_loss: 168.0897 - val_mse: 168.0897\n",
      "Epoch 277/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 77.6719 - mse: 77.6719 - val_loss: 165.0567 - val_mse: 165.0567\n",
      "Epoch 278/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 77.6628 - mse: 77.6628 - val_loss: 166.6920 - val_mse: 166.6920\n",
      "Epoch 279/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 76.7831 - mse: 76.7831 - val_loss: 168.7422 - val_mse: 168.7422\n",
      "Epoch 280/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 77.4319 - mse: 77.4319 - val_loss: 166.0017 - val_mse: 166.0017\n",
      "Epoch 281/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 78.1712 - mse: 78.1712 - val_loss: 165.1871 - val_mse: 165.1871\n",
      "Epoch 282/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 75.4244 - mse: 75.4244 - val_loss: 171.2163 - val_mse: 171.2163\n",
      "Epoch 283/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 77.9150 - mse: 77.9150 - val_loss: 169.3835 - val_mse: 169.3835\n",
      "Epoch 284/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 76.2761 - mse: 76.2761 - val_loss: 164.6998 - val_mse: 164.6998\n",
      "Epoch 285/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 76.3245 - mse: 76.3245 - val_loss: 165.8191 - val_mse: 165.8191\n",
      "Epoch 286/350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 4ms/step - loss: 75.7112 - mse: 75.7112 - val_loss: 166.5440 - val_mse: 166.5440\n",
      "Epoch 287/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 75.5133 - mse: 75.5133 - val_loss: 165.3074 - val_mse: 165.3074\n",
      "Epoch 288/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 75.2385 - mse: 75.2385 - val_loss: 164.4405 - val_mse: 164.4405\n",
      "Epoch 289/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 75.2793 - mse: 75.2793 - val_loss: 166.5027 - val_mse: 166.5027\n",
      "Epoch 290/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 76.4355 - mse: 76.4355 - val_loss: 166.4362 - val_mse: 166.4362\n",
      "Epoch 291/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 75.6622 - mse: 75.6622 - val_loss: 164.3484 - val_mse: 164.3484\n",
      "Epoch 292/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 74.1428 - mse: 74.1428 - val_loss: 167.6345 - val_mse: 167.6345\n",
      "Epoch 293/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 74.5768 - mse: 74.5768 - val_loss: 166.5246 - val_mse: 166.5246\n",
      "Epoch 294/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 76.2143 - mse: 76.2143 - val_loss: 164.7150 - val_mse: 164.7150\n",
      "Epoch 295/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 74.7425 - mse: 74.7425 - val_loss: 168.6604 - val_mse: 168.6604\n",
      "Epoch 296/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 73.2978 - mse: 73.2978 - val_loss: 166.4717 - val_mse: 166.4717\n",
      "Epoch 297/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 75.7996 - mse: 75.7996 - val_loss: 165.2375 - val_mse: 165.2375\n",
      "Epoch 298/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 73.8915 - mse: 73.8915 - val_loss: 164.9837 - val_mse: 164.9837\n",
      "Epoch 299/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 73.4105 - mse: 73.4105 - val_loss: 165.8561 - val_mse: 165.8561\n",
      "Epoch 300/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 73.2152 - mse: 73.2152 - val_loss: 165.3817 - val_mse: 165.3817\n",
      "Epoch 301/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 74.7041 - mse: 74.7041 - val_loss: 165.0515 - val_mse: 165.0515\n",
      "Epoch 302/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 73.1359 - mse: 73.1359 - val_loss: 170.4587 - val_mse: 170.4587\n",
      "Epoch 303/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 73.8779 - mse: 73.8779 - val_loss: 163.3404 - val_mse: 163.3404\n",
      "Epoch 304/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 72.5588 - mse: 72.5588 - val_loss: 165.0383 - val_mse: 165.0383\n",
      "Epoch 305/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 73.6902 - mse: 73.6902 - val_loss: 167.2576 - val_mse: 167.2576\n",
      "Epoch 306/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 72.6817 - mse: 72.6817 - val_loss: 163.5487 - val_mse: 163.5487\n",
      "Epoch 307/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 72.3116 - mse: 72.3116 - val_loss: 166.1376 - val_mse: 166.1376\n",
      "Epoch 308/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 71.9541 - mse: 71.9541 - val_loss: 163.7788 - val_mse: 163.7788\n",
      "Epoch 309/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 71.8114 - mse: 71.8114 - val_loss: 163.3411 - val_mse: 163.3411\n",
      "Epoch 310/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 73.0283 - mse: 73.0283 - val_loss: 164.8791 - val_mse: 164.8791\n",
      "Epoch 311/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 72.4468 - mse: 72.4468 - val_loss: 163.1572 - val_mse: 163.1572\n",
      "Epoch 312/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 71.0645 - mse: 71.0645 - val_loss: 165.0327 - val_mse: 165.0327\n",
      "Epoch 313/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 70.8587 - mse: 70.8587 - val_loss: 162.0877 - val_mse: 162.0877\n",
      "Epoch 314/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 71.0178 - mse: 71.0178 - val_loss: 162.9590 - val_mse: 162.9590\n",
      "Epoch 315/350\n",
      "10/10 [==============================] - ETA: 0s - loss: 28.8740 - mse: 28.874 - 0s 4ms/step - loss: 70.8468 - mse: 70.8468 - val_loss: 164.6013 - val_mse: 164.6013\n",
      "Epoch 316/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 70.4635 - mse: 70.4635 - val_loss: 163.9975 - val_mse: 163.9975\n",
      "Epoch 317/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 70.6710 - mse: 70.6710 - val_loss: 164.4650 - val_mse: 164.4650\n",
      "Epoch 318/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 70.3390 - mse: 70.3390 - val_loss: 163.4323 - val_mse: 163.4323\n",
      "Epoch 319/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 72.0848 - mse: 72.0848 - val_loss: 164.1340 - val_mse: 164.1340\n",
      "Epoch 320/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 72.5277 - mse: 72.5277 - val_loss: 161.2432 - val_mse: 161.2432\n",
      "Epoch 321/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 68.8401 - mse: 68.8401 - val_loss: 167.9071 - val_mse: 167.9071\n",
      "Epoch 322/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 69.7955 - mse: 69.7955 - val_loss: 162.8304 - val_mse: 162.8304\n",
      "Epoch 323/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 70.6534 - mse: 70.6534 - val_loss: 162.5751 - val_mse: 162.5751\n",
      "Epoch 324/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 70.0257 - mse: 70.0257 - val_loss: 165.8348 - val_mse: 165.8348\n",
      "Epoch 325/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 72.1761 - mse: 72.1761 - val_loss: 160.9681 - val_mse: 160.9681\n",
      "Epoch 326/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 68.4229 - mse: 68.4229 - val_loss: 168.3768 - val_mse: 168.3768\n",
      "Epoch 327/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 69.3393 - mse: 69.3393 - val_loss: 161.7979 - val_mse: 161.7979\n",
      "Epoch 328/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 68.9932 - mse: 68.9932 - val_loss: 164.6366 - val_mse: 164.6366\n",
      "Epoch 329/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 68.7746 - mse: 68.7746 - val_loss: 162.7294 - val_mse: 162.7294\n",
      "Epoch 330/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 68.0680 - mse: 68.0680 - val_loss: 161.9793 - val_mse: 161.9793\n",
      "Epoch 331/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 68.0850 - mse: 68.0850 - val_loss: 163.3486 - val_mse: 163.3486\n",
      "Epoch 332/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 68.0061 - mse: 68.0061 - val_loss: 162.9796 - val_mse: 162.9796\n",
      "Epoch 333/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 68.0047 - mse: 68.0047 - val_loss: 161.3331 - val_mse: 161.3331\n",
      "Epoch 334/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 67.1099 - mse: 67.1099 - val_loss: 165.7547 - val_mse: 165.7547\n",
      "Epoch 335/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 67.7982 - mse: 67.7982 - val_loss: 162.2500 - val_mse: 162.2500\n",
      "Epoch 336/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 69.4795 - mse: 69.4795 - val_loss: 161.1492 - val_mse: 161.1492\n",
      "Epoch 337/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 69.1784 - mse: 69.1784 - val_loss: 168.3150 - val_mse: 168.3150\n",
      "Epoch 338/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 65.8638 - mse: 65.8638 - val_loss: 162.5213 - val_mse: 162.5213\n",
      "Epoch 339/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 67.9344 - mse: 67.9344 - val_loss: 163.3540 - val_mse: 163.3540\n",
      "Epoch 340/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 67.3420 - mse: 67.3420 - val_loss: 165.3695 - val_mse: 165.3695\n",
      "Epoch 341/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 65.7670 - mse: 65.7670 - val_loss: 162.5294 - val_mse: 162.5294\n",
      "Epoch 342/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 68.4053 - mse: 68.4053 - val_loss: 161.3677 - val_mse: 161.3677\n",
      "Epoch 343/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 65.9864 - mse: 65.9864 - val_loss: 164.6663 - val_mse: 164.6663\n",
      "Epoch 344/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 65.7609 - mse: 65.7609 - val_loss: 163.8286 - val_mse: 163.8286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 345/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 65.1192 - mse: 65.1192 - val_loss: 164.1487 - val_mse: 164.1487\n",
      "Epoch 346/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 65.3622 - mse: 65.3622 - val_loss: 164.1676 - val_mse: 164.1676\n",
      "Epoch 347/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 65.0129 - mse: 65.0129 - val_loss: 165.0635 - val_mse: 165.0635\n",
      "Epoch 348/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 64.5397 - mse: 64.5397 - val_loss: 164.5448 - val_mse: 164.5448\n",
      "Epoch 349/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 64.7102 - mse: 64.7102 - val_loss: 165.1895 - val_mse: 165.1895\n",
      "Epoch 350/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 64.4426 - mse: 64.4426 - val_loss: 163.8446 - val_mse: 163.8446\n",
      "Epoch 1/350\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 16082.3857 - mse: 16082.3857 - val_loss: 16060.8018 - val_mse: 16060.8018\n",
      "Epoch 2/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 15951.0996 - mse: 15951.0996 - val_loss: 15927.4307 - val_mse: 15927.4307\n",
      "Epoch 3/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 15817.7549 - mse: 15817.7549 - val_loss: 15771.0928 - val_mse: 15771.0928\n",
      "Epoch 4/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 15644.8682 - mse: 15644.8682 - val_loss: 15550.5264 - val_mse: 15550.5264\n",
      "Epoch 5/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 15388.0703 - mse: 15388.0703 - val_loss: 15222.0273 - val_mse: 15222.0273\n",
      "Epoch 6/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 14996.6582 - mse: 14996.6582 - val_loss: 14726.5439 - val_mse: 14726.5439\n",
      "Epoch 7/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 14402.1777 - mse: 14402.1777 - val_loss: 13993.1797 - val_mse: 13993.1797\n",
      "Epoch 8/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 13553.0410 - mse: 13553.0410 - val_loss: 12923.4072 - val_mse: 12923.4072\n",
      "Epoch 9/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 12303.0352 - mse: 12303.0352 - val_loss: 11468.9922 - val_mse: 11468.9922\n",
      "Epoch 10/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 10644.4854 - mse: 10644.4854 - val_loss: 9640.9199 - val_mse: 9640.9199\n",
      "Epoch 11/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 8671.5986 - mse: 8671.5986 - val_loss: 7563.9673 - val_mse: 7563.9673\n",
      "Epoch 12/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 6406.1514 - mse: 6406.1514 - val_loss: 5602.9941 - val_mse: 5602.9941\n",
      "Epoch 13/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 4452.2983 - mse: 4452.2983 - val_loss: 4122.4312 - val_mse: 4122.4312\n",
      "Epoch 14/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 3081.8298 - mse: 3081.8298 - val_loss: 3306.9128 - val_mse: 3306.9128\n",
      "Epoch 15/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 2391.4043 - mse: 2391.4041 - val_loss: 2862.8311 - val_mse: 2862.8311\n",
      "Epoch 16/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 2080.0789 - mse: 2080.0786 - val_loss: 2593.7402 - val_mse: 2593.7402\n",
      "Epoch 17/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 1911.1139 - mse: 1911.1139 - val_loss: 2418.6414 - val_mse: 2418.6414\n",
      "Epoch 18/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 1796.3143 - mse: 1796.3143 - val_loss: 2300.5728 - val_mse: 2300.5728\n",
      "Epoch 19/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 1711.4740 - mse: 1711.4740 - val_loss: 2196.5237 - val_mse: 2196.5237\n",
      "Epoch 20/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 1629.1934 - mse: 1629.1934 - val_loss: 2093.2424 - val_mse: 2093.2424\n",
      "Epoch 21/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 1545.4946 - mse: 1545.4946 - val_loss: 1996.1595 - val_mse: 1996.1595\n",
      "Epoch 22/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 1473.5743 - mse: 1473.5743 - val_loss: 1906.1171 - val_mse: 1906.1171\n",
      "Epoch 23/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 1411.1283 - mse: 1411.1283 - val_loss: 1835.2305 - val_mse: 1835.2305\n",
      "Epoch 24/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 1350.6140 - mse: 1350.6140 - val_loss: 1763.3224 - val_mse: 1763.3224\n",
      "Epoch 25/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 1297.8333 - mse: 1297.8333 - val_loss: 1706.7885 - val_mse: 1706.7885\n",
      "Epoch 26/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 1242.8162 - mse: 1242.8162 - val_loss: 1655.4150 - val_mse: 1655.4150\n",
      "Epoch 27/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 1203.3054 - mse: 1203.3054 - val_loss: 1597.1603 - val_mse: 1597.1603\n",
      "Epoch 28/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 1155.4456 - mse: 1155.4456 - val_loss: 1529.2021 - val_mse: 1529.2021\n",
      "Epoch 29/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 1116.1903 - mse: 1116.1903 - val_loss: 1478.1235 - val_mse: 1478.1235\n",
      "Epoch 30/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 1075.7942 - mse: 1075.7942 - val_loss: 1418.9926 - val_mse: 1418.9926\n",
      "Epoch 31/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 1037.8181 - mse: 1037.8181 - val_loss: 1382.1016 - val_mse: 1382.1016\n",
      "Epoch 32/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 1002.8959 - mse: 1002.8959 - val_loss: 1340.7003 - val_mse: 1340.7003\n",
      "Epoch 33/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 971.3174 - mse: 971.3174 - val_loss: 1301.8197 - val_mse: 1301.8197\n",
      "Epoch 34/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 941.9536 - mse: 941.9536 - val_loss: 1277.1519 - val_mse: 1277.1519\n",
      "Epoch 35/350\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 911.6036 - mse: 911.6036 - val_loss: 1230.1205 - val_mse: 1230.1205\n",
      "Epoch 36/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 880.8647 - mse: 880.8647 - val_loss: 1198.4419 - val_mse: 1198.4419\n",
      "Epoch 37/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 855.6073 - mse: 855.6073 - val_loss: 1168.3524 - val_mse: 1168.3524\n",
      "Epoch 38/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 829.9170 - mse: 829.9170 - val_loss: 1139.8555 - val_mse: 1139.8555\n",
      "Epoch 39/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 805.1276 - mse: 805.1276 - val_loss: 1113.1815 - val_mse: 1113.1815\n",
      "Epoch 40/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 780.8532 - mse: 780.8532 - val_loss: 1087.9702 - val_mse: 1087.9702\n",
      "Epoch 41/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 760.1178 - mse: 760.1177 - val_loss: 1066.5002 - val_mse: 1066.5002\n",
      "Epoch 42/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 738.3408 - mse: 738.3408 - val_loss: 1040.8094 - val_mse: 1040.8094\n",
      "Epoch 43/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 718.9464 - mse: 718.9464 - val_loss: 1026.3048 - val_mse: 1026.3048\n",
      "Epoch 44/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 699.3281 - mse: 699.3281 - val_loss: 1001.5039 - val_mse: 1001.5039\n",
      "Epoch 45/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 681.4847 - mse: 681.4846 - val_loss: 977.3663 - val_mse: 977.3662\n",
      "Epoch 46/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 660.0850 - mse: 660.0850 - val_loss: 962.9815 - val_mse: 962.9815\n",
      "Epoch 47/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 644.3817 - mse: 644.3817 - val_loss: 946.0972 - val_mse: 946.0972\n",
      "Epoch 48/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 625.8798 - mse: 625.8798 - val_loss: 926.5001 - val_mse: 926.5001\n",
      "Epoch 49/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 611.1475 - mse: 611.1475 - val_loss: 904.8636 - val_mse: 904.8636\n",
      "Epoch 50/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 594.5453 - mse: 594.5453 - val_loss: 891.7042 - val_mse: 891.7042\n",
      "Epoch 51/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 580.0236 - mse: 580.0236 - val_loss: 881.6334 - val_mse: 881.6334\n",
      "Epoch 52/350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 4ms/step - loss: 566.7900 - mse: 566.7900 - val_loss: 852.4362 - val_mse: 852.4362\n",
      "Epoch 53/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 549.6546 - mse: 549.6546 - val_loss: 853.6115 - val_mse: 853.6115\n",
      "Epoch 54/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 532.6443 - mse: 532.6444 - val_loss: 829.5773 - val_mse: 829.5773\n",
      "Epoch 55/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 519.0925 - mse: 519.0925 - val_loss: 809.7578 - val_mse: 809.7578\n",
      "Epoch 56/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 507.4750 - mse: 507.4750 - val_loss: 794.5438 - val_mse: 794.5438\n",
      "Epoch 57/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 494.7922 - mse: 494.7922 - val_loss: 793.0588 - val_mse: 793.0588\n",
      "Epoch 58/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 481.6591 - mse: 481.6591 - val_loss: 782.9697 - val_mse: 782.9697\n",
      "Epoch 59/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 470.5804 - mse: 470.5804 - val_loss: 764.4720 - val_mse: 764.4720\n",
      "Epoch 60/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 460.8375 - mse: 460.8375 - val_loss: 746.8587 - val_mse: 746.8587\n",
      "Epoch 61/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 449.3467 - mse: 449.3467 - val_loss: 740.9133 - val_mse: 740.9133\n",
      "Epoch 62/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 440.0484 - mse: 440.0484 - val_loss: 732.2541 - val_mse: 732.2541\n",
      "Epoch 63/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 428.8446 - mse: 428.8446 - val_loss: 715.0435 - val_mse: 715.0435\n",
      "Epoch 64/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 419.1232 - mse: 419.1232 - val_loss: 700.6094 - val_mse: 700.6094\n",
      "Epoch 65/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 410.8649 - mse: 410.8649 - val_loss: 693.5562 - val_mse: 693.5562\n",
      "Epoch 66/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 402.4948 - mse: 402.4948 - val_loss: 689.0984 - val_mse: 689.0984\n",
      "Epoch 67/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 399.1774 - mse: 399.1774 - val_loss: 685.7344 - val_mse: 685.7344\n",
      "Epoch 68/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 388.4742 - mse: 388.4742 - val_loss: 651.4610 - val_mse: 651.4610\n",
      "Epoch 69/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 378.9339 - mse: 378.9339 - val_loss: 647.1193 - val_mse: 647.1193\n",
      "Epoch 70/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 369.6957 - mse: 369.6957 - val_loss: 648.2114 - val_mse: 648.2114\n",
      "Epoch 71/350\n",
      "10/10 [==============================] - ETA: 0s - loss: 364.7066 - mse: 364.706 - 0s 3ms/step - loss: 363.2667 - mse: 363.2667 - val_loss: 638.5804 - val_mse: 638.5804\n",
      "Epoch 72/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 355.7978 - mse: 355.7978 - val_loss: 625.8754 - val_mse: 625.8754\n",
      "Epoch 73/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 351.0972 - mse: 351.0972 - val_loss: 624.5102 - val_mse: 624.5102\n",
      "Epoch 74/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 343.0220 - mse: 343.0220 - val_loss: 603.3336 - val_mse: 603.3336\n",
      "Epoch 75/350\n",
      "10/10 [==============================] - ETA: 0s - loss: 308.5077 - mse: 308.507 - 0s 3ms/step - loss: 336.0154 - mse: 336.0154 - val_loss: 596.3253 - val_mse: 596.3253\n",
      "Epoch 76/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 328.6480 - mse: 328.6480 - val_loss: 588.7334 - val_mse: 588.7334\n",
      "Epoch 77/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 323.6028 - mse: 323.6028 - val_loss: 590.3382 - val_mse: 590.3382\n",
      "Epoch 78/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 317.5912 - mse: 317.5912 - val_loss: 576.6505 - val_mse: 576.6505\n",
      "Epoch 79/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 311.6471 - mse: 311.6471 - val_loss: 570.1840 - val_mse: 570.1840\n",
      "Epoch 80/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 306.4857 - mse: 306.4857 - val_loss: 569.6531 - val_mse: 569.6531\n",
      "Epoch 81/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 300.9313 - mse: 300.9313 - val_loss: 552.8578 - val_mse: 552.8578\n",
      "Epoch 82/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 295.3169 - mse: 295.3169 - val_loss: 552.5908 - val_mse: 552.5908\n",
      "Epoch 83/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 289.5985 - mse: 289.5985 - val_loss: 545.7267 - val_mse: 545.7267\n",
      "Epoch 84/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 284.1795 - mse: 284.1795 - val_loss: 542.2720 - val_mse: 542.2720\n",
      "Epoch 85/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 280.1957 - mse: 280.1957 - val_loss: 540.4958 - val_mse: 540.4958\n",
      "Epoch 86/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 274.6107 - mse: 274.6107 - val_loss: 521.7720 - val_mse: 521.7720\n",
      "Epoch 87/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 269.7809 - mse: 269.7809 - val_loss: 517.6218 - val_mse: 517.6218\n",
      "Epoch 88/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 265.4336 - mse: 265.4336 - val_loss: 521.5825 - val_mse: 521.5825\n",
      "Epoch 89/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 261.8110 - mse: 261.8110 - val_loss: 513.7438 - val_mse: 513.7438\n",
      "Epoch 90/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 257.1525 - mse: 257.1525 - val_loss: 497.4826 - val_mse: 497.4826\n",
      "Epoch 91/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 253.7959 - mse: 253.7959 - val_loss: 495.1206 - val_mse: 495.1206\n",
      "Epoch 92/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 248.8733 - mse: 248.8733 - val_loss: 496.2744 - val_mse: 496.2744\n",
      "Epoch 93/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 246.3206 - mse: 246.3206 - val_loss: 494.3244 - val_mse: 494.3244\n",
      "Epoch 94/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 242.0103 - mse: 242.0103 - val_loss: 479.9219 - val_mse: 479.9219\n",
      "Epoch 95/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 238.4817 - mse: 238.4817 - val_loss: 478.9905 - val_mse: 478.9905\n",
      "Epoch 96/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 234.4269 - mse: 234.4269 - val_loss: 478.8071 - val_mse: 478.8071\n",
      "Epoch 97/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 230.8085 - mse: 230.8085 - val_loss: 472.2765 - val_mse: 472.2765\n",
      "Epoch 98/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 227.3880 - mse: 227.3880 - val_loss: 468.1619 - val_mse: 468.1619\n",
      "Epoch 99/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 224.3318 - mse: 224.3318 - val_loss: 464.6927 - val_mse: 464.6927\n",
      "Epoch 100/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 221.3272 - mse: 221.3272 - val_loss: 458.0365 - val_mse: 458.0365\n",
      "Epoch 101/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 221.0848 - mse: 221.0848 - val_loss: 459.0606 - val_mse: 459.0606\n",
      "Epoch 102/350\n",
      "10/10 [==============================] - ETA: 0s - loss: 232.7723 - mse: 232.772 - 0s 3ms/step - loss: 215.2786 - mse: 215.2786 - val_loss: 445.9916 - val_mse: 445.9916\n",
      "Epoch 103/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 213.3373 - mse: 213.3373 - val_loss: 443.4037 - val_mse: 443.4037\n",
      "Epoch 104/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 210.1605 - mse: 210.1605 - val_loss: 439.2233 - val_mse: 439.2233\n",
      "Epoch 105/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 207.8245 - mse: 207.8245 - val_loss: 443.3255 - val_mse: 443.3255\n",
      "Epoch 106/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 204.6991 - mse: 204.6991 - val_loss: 435.7321 - val_mse: 435.7321\n",
      "Epoch 107/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 202.6780 - mse: 202.6780 - val_loss: 433.1735 - val_mse: 433.1735\n",
      "Epoch 108/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 199.1819 - mse: 199.1819 - val_loss: 426.3321 - val_mse: 426.3321\n",
      "Epoch 109/350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 3ms/step - loss: 197.0411 - mse: 197.0412 - val_loss: 421.1100 - val_mse: 421.1100\n",
      "Epoch 110/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 194.8057 - mse: 194.8057 - val_loss: 422.4283 - val_mse: 422.4283\n",
      "Epoch 111/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 192.6754 - mse: 192.6754 - val_loss: 417.5183 - val_mse: 417.5183\n",
      "Epoch 112/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 189.8822 - mse: 189.8822 - val_loss: 412.8363 - val_mse: 412.8363\n",
      "Epoch 113/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 187.1240 - mse: 187.1240 - val_loss: 410.2346 - val_mse: 410.2346\n",
      "Epoch 114/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 185.7592 - mse: 185.7592 - val_loss: 410.9656 - val_mse: 410.9656\n",
      "Epoch 115/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 182.3022 - mse: 182.3022 - val_loss: 403.6392 - val_mse: 403.6392\n",
      "Epoch 116/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 180.9511 - mse: 180.9511 - val_loss: 399.2488 - val_mse: 399.2488\n",
      "Epoch 117/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 179.1088 - mse: 179.1088 - val_loss: 405.5868 - val_mse: 405.5868\n",
      "Epoch 118/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 176.5451 - mse: 176.5451 - val_loss: 397.5305 - val_mse: 397.5305\n",
      "Epoch 119/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 174.5841 - mse: 174.5841 - val_loss: 392.7168 - val_mse: 392.7168\n",
      "Epoch 120/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 172.8517 - mse: 172.8517 - val_loss: 393.9497 - val_mse: 393.9497\n",
      "Epoch 121/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 171.2863 - mse: 171.2863 - val_loss: 390.2284 - val_mse: 390.2284\n",
      "Epoch 122/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 168.8280 - mse: 168.8280 - val_loss: 385.7733 - val_mse: 385.7733\n",
      "Epoch 123/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 167.2087 - mse: 167.2087 - val_loss: 384.3213 - val_mse: 384.3213\n",
      "Epoch 124/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 165.2601 - mse: 165.2601 - val_loss: 385.4618 - val_mse: 385.4618\n",
      "Epoch 125/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 163.5359 - mse: 163.5359 - val_loss: 377.2570 - val_mse: 377.2570\n",
      "Epoch 126/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 161.6767 - mse: 161.6767 - val_loss: 373.8125 - val_mse: 373.8125\n",
      "Epoch 127/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 160.2840 - mse: 160.2840 - val_loss: 375.1220 - val_mse: 375.1220\n",
      "Epoch 128/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 157.9471 - mse: 157.9471 - val_loss: 368.1897 - val_mse: 368.1897\n",
      "Epoch 129/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 156.9645 - mse: 156.9645 - val_loss: 363.9083 - val_mse: 363.9083\n",
      "Epoch 130/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 155.7917 - mse: 155.7917 - val_loss: 363.0731 - val_mse: 363.0731\n",
      "Epoch 131/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 154.0480 - mse: 154.0480 - val_loss: 365.0859 - val_mse: 365.0859\n",
      "Epoch 132/350\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 151.8436 - mse: 151.8436 - val_loss: 358.6835 - val_mse: 358.6835\n",
      "Epoch 133/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 152.3290 - mse: 152.3290 - val_loss: 353.7729 - val_mse: 353.7729\n",
      "Epoch 134/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 150.1103 - mse: 150.1103 - val_loss: 358.1017 - val_mse: 358.1017\n",
      "Epoch 135/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 148.7341 - mse: 148.7341 - val_loss: 352.4479 - val_mse: 352.4479\n",
      "Epoch 136/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 146.9359 - mse: 146.9359 - val_loss: 352.0210 - val_mse: 352.0210\n",
      "Epoch 137/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 144.2637 - mse: 144.2637 - val_loss: 345.5411 - val_mse: 345.5411\n",
      "Epoch 138/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 143.5205 - mse: 143.5205 - val_loss: 342.8853 - val_mse: 342.8853\n",
      "Epoch 139/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 142.9758 - mse: 142.9758 - val_loss: 339.7557 - val_mse: 339.7557\n",
      "Epoch 140/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 140.9448 - mse: 140.9448 - val_loss: 339.3238 - val_mse: 339.3238\n",
      "Epoch 141/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 140.3026 - mse: 140.3026 - val_loss: 341.5770 - val_mse: 341.5770\n",
      "Epoch 142/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 138.7557 - mse: 138.7557 - val_loss: 333.4294 - val_mse: 333.4294\n",
      "Epoch 143/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 137.4229 - mse: 137.4229 - val_loss: 331.6556 - val_mse: 331.6556\n",
      "Epoch 144/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 135.6417 - mse: 135.6417 - val_loss: 333.1431 - val_mse: 333.1431\n",
      "Epoch 145/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 134.6600 - mse: 134.6600 - val_loss: 328.6996 - val_mse: 328.6996\n",
      "Epoch 146/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 134.4234 - mse: 134.4235 - val_loss: 325.6122 - val_mse: 325.6122\n",
      "Epoch 147/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 132.2876 - mse: 132.2877 - val_loss: 326.2833 - val_mse: 326.2833\n",
      "Epoch 148/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 131.7525 - mse: 131.7525 - val_loss: 329.2928 - val_mse: 329.2928\n",
      "Epoch 149/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 130.0831 - mse: 130.0831 - val_loss: 320.4877 - val_mse: 320.4877\n",
      "Epoch 150/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 129.0765 - mse: 129.0765 - val_loss: 317.1756 - val_mse: 317.1756\n",
      "Epoch 151/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 128.5338 - mse: 128.5338 - val_loss: 322.1561 - val_mse: 322.1561\n",
      "Epoch 152/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 126.2670 - mse: 126.2670 - val_loss: 315.7149 - val_mse: 315.7149\n",
      "Epoch 153/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 125.7644 - mse: 125.7644 - val_loss: 313.2245 - val_mse: 313.2245\n",
      "Epoch 154/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 125.3093 - mse: 125.3093 - val_loss: 316.0439 - val_mse: 316.0439\n",
      "Epoch 155/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 123.3172 - mse: 123.3172 - val_loss: 311.0998 - val_mse: 311.0998\n",
      "Epoch 156/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 123.1093 - mse: 123.1093 - val_loss: 309.4621 - val_mse: 309.4621\n",
      "Epoch 157/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 121.8221 - mse: 121.8221 - val_loss: 309.6526 - val_mse: 309.6526\n",
      "Epoch 158/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 120.8738 - mse: 120.8738 - val_loss: 309.3910 - val_mse: 309.3910\n",
      "Epoch 159/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 121.0952 - mse: 121.0953 - val_loss: 306.4325 - val_mse: 306.4325\n",
      "Epoch 160/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 120.3479 - mse: 120.3479 - val_loss: 300.8987 - val_mse: 300.8987\n",
      "Epoch 161/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 118.6927 - mse: 118.6927 - val_loss: 306.1295 - val_mse: 306.1295\n",
      "Epoch 162/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 117.1579 - mse: 117.1579 - val_loss: 299.6447 - val_mse: 299.6447\n",
      "Epoch 163/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 116.9339 - mse: 116.9339 - val_loss: 297.2532 - val_mse: 297.2532\n",
      "Epoch 164/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 115.5831 - mse: 115.5831 - val_loss: 298.0122 - val_mse: 298.0122\n",
      "Epoch 165/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 115.0274 - mse: 115.0274 - val_loss: 301.6898 - val_mse: 301.6898\n",
      "Epoch 166/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 114.6967 - mse: 114.6967 - val_loss: 300.0012 - val_mse: 300.0012\n",
      "Epoch 167/350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 7ms/step - loss: 115.8548 - mse: 115.8548 - val_loss: 292.1761 - val_mse: 292.1761\n",
      "Epoch 168/350\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 112.3639 - mse: 112.3639 - val_loss: 296.8695 - val_mse: 296.8695\n",
      "Epoch 169/350\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 111.9463 - mse: 111.9463 - val_loss: 293.9308 - val_mse: 293.9308\n",
      "Epoch 170/350\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 110.9626 - mse: 110.9626 - val_loss: 293.4445 - val_mse: 293.4445\n",
      "Epoch 171/350\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 110.5792 - mse: 110.5792 - val_loss: 293.2411 - val_mse: 293.2411\n",
      "Epoch 172/350\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 110.2050 - mse: 110.2050 - val_loss: 291.6106 - val_mse: 291.6106\n",
      "Epoch 173/350\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 108.7127 - mse: 108.7127 - val_loss: 291.6069 - val_mse: 291.6069\n",
      "Epoch 174/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 109.5120 - mse: 109.5120 - val_loss: 285.8192 - val_mse: 285.8192\n",
      "Epoch 175/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 110.4101 - mse: 110.4101 - val_loss: 291.6989 - val_mse: 291.6989\n",
      "Epoch 176/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 107.5109 - mse: 107.5109 - val_loss: 282.3830 - val_mse: 282.3830\n",
      "Epoch 177/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 106.7769 - mse: 106.7769 - val_loss: 285.2719 - val_mse: 285.2719\n",
      "Epoch 178/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 105.4309 - mse: 105.4309 - val_loss: 285.3997 - val_mse: 285.3997\n",
      "Epoch 179/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 104.7057 - mse: 104.7057 - val_loss: 282.0945 - val_mse: 282.0945\n",
      "Epoch 180/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 106.4876 - mse: 106.4876 - val_loss: 278.3541 - val_mse: 278.3541\n",
      "Epoch 181/350\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 103.6429 - mse: 103.6429 - val_loss: 285.6464 - val_mse: 285.6464\n",
      "Epoch 182/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 103.8239 - mse: 103.8239 - val_loss: 279.3103 - val_mse: 279.3103\n",
      "Epoch 183/350\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 104.8639 - mse: 104.8639 - val_loss: 280.3879 - val_mse: 280.3879\n",
      "Epoch 184/350\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 100.8817 - mse: 100.8817 - val_loss: 273.9674 - val_mse: 273.9674\n",
      "Epoch 185/350\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 102.3227 - mse: 102.3227 - val_loss: 273.6133 - val_mse: 273.6133\n",
      "Epoch 186/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 101.4470 - mse: 101.4470 - val_loss: 282.4330 - val_mse: 282.4330\n",
      "Epoch 187/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 100.6200 - mse: 100.6200 - val_loss: 276.3307 - val_mse: 276.3307\n",
      "Epoch 188/350\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 99.2659 - mse: 99.2659 - val_loss: 271.3883 - val_mse: 271.3883\n",
      "Epoch 189/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 99.3268 - mse: 99.3268 - val_loss: 272.7762 - val_mse: 272.7762\n",
      "Epoch 190/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 97.9663 - mse: 97.9663 - val_loss: 276.2803 - val_mse: 276.2803\n",
      "Epoch 191/350\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 98.0662 - mse: 98.0662 - val_loss: 272.5341 - val_mse: 272.5341\n",
      "Epoch 192/350\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 97.0564 - mse: 97.0564 - val_loss: 270.3586 - val_mse: 270.3586\n",
      "Epoch 193/350\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 98.3270 - mse: 98.3270 - val_loss: 269.6100 - val_mse: 269.6100\n",
      "Epoch 194/350\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 96.3977 - mse: 96.3977 - val_loss: 273.1403 - val_mse: 273.1403\n",
      "Epoch 195/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 96.8946 - mse: 96.8946 - val_loss: 271.5146 - val_mse: 271.5146\n",
      "Epoch 196/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 97.3919 - mse: 97.3919 - val_loss: 264.8383 - val_mse: 264.8383\n",
      "Epoch 197/350\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 95.7001 - mse: 95.7001 - val_loss: 270.7683 - val_mse: 270.7683\n",
      "Epoch 198/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 94.7184 - mse: 94.7184 - val_loss: 267.9751 - val_mse: 267.9751\n",
      "Epoch 199/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 94.2797 - mse: 94.2797 - val_loss: 265.3852 - val_mse: 265.3852\n",
      "Epoch 200/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 94.1375 - mse: 94.1375 - val_loss: 269.3867 - val_mse: 269.3867\n",
      "Epoch 201/350\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 93.5905 - mse: 93.5905 - val_loss: 266.4682 - val_mse: 266.4682\n",
      "Epoch 202/350\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 93.5413 - mse: 93.5413 - val_loss: 265.7972 - val_mse: 265.7972\n",
      "Epoch 203/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 93.4226 - mse: 93.4226 - val_loss: 268.2374 - val_mse: 268.2373\n",
      "Epoch 204/350\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 92.0672 - mse: 92.0672 - val_loss: 263.8846 - val_mse: 263.8846\n",
      "Epoch 205/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 91.7038 - mse: 91.7038 - val_loss: 262.7821 - val_mse: 262.7821\n",
      "Epoch 206/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 90.6851 - mse: 90.6851 - val_loss: 265.8995 - val_mse: 265.8995\n",
      "Epoch 207/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 91.8679 - mse: 91.8679 - val_loss: 261.9680 - val_mse: 261.9680\n",
      "Epoch 208/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 90.3733 - mse: 90.3733 - val_loss: 261.6998 - val_mse: 261.6998\n",
      "Epoch 209/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 90.1131 - mse: 90.1131 - val_loss: 262.3588 - val_mse: 262.3588\n",
      "Epoch 210/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 90.0279 - mse: 90.0279 - val_loss: 259.2461 - val_mse: 259.2461\n",
      "Epoch 211/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 89.3524 - mse: 89.3524 - val_loss: 262.9781 - val_mse: 262.9781\n",
      "Epoch 212/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 88.4568 - mse: 88.4568 - val_loss: 260.3978 - val_mse: 260.3978\n",
      "Epoch 213/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 88.0392 - mse: 88.0392 - val_loss: 260.4550 - val_mse: 260.4550\n",
      "Epoch 214/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 87.9863 - mse: 87.9863 - val_loss: 259.9167 - val_mse: 259.9167\n",
      "Epoch 215/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 88.4083 - mse: 88.4083 - val_loss: 256.7843 - val_mse: 256.7843\n",
      "Epoch 216/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 87.7643 - mse: 87.7643 - val_loss: 261.0123 - val_mse: 261.0123\n",
      "Epoch 217/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 87.7768 - mse: 87.7768 - val_loss: 257.1330 - val_mse: 257.1330\n",
      "Epoch 218/350\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 86.2743 - mse: 86.2743 - val_loss: 259.1471 - val_mse: 259.1471\n",
      "Epoch 219/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 86.3106 - mse: 86.3106 - val_loss: 256.9963 - val_mse: 256.9963\n",
      "Epoch 220/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 86.6869 - mse: 86.6869 - val_loss: 259.4447 - val_mse: 259.4447\n",
      "Epoch 221/350\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 85.4334 - mse: 85.4334 - val_loss: 258.9652 - val_mse: 258.9652\n",
      "Epoch 222/350\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 85.1518 - mse: 85.1518 - val_loss: 258.5208 - val_mse: 258.5208\n",
      "Epoch 223/350\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 84.3300 - mse: 84.3300 - val_loss: 258.1181 - val_mse: 258.1181\n",
      "Epoch 224/350\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 84.0463 - mse: 84.0463 - val_loss: 257.2199 - val_mse: 257.2199\n",
      "Epoch 225/350\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 83.9143 - mse: 83.9143 - val_loss: 257.6397 - val_mse: 257.6397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 226/350\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 83.8216 - mse: 83.8216 - val_loss: 257.5805 - val_mse: 257.5805\n",
      "Epoch 227/350\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 83.6975 - mse: 83.6975 - val_loss: 260.2350 - val_mse: 260.2350\n",
      "Epoch 228/350\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 84.8991 - mse: 84.8991 - val_loss: 254.9622 - val_mse: 254.9622\n",
      "Epoch 229/350\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 82.9376 - mse: 82.9376 - val_loss: 258.7854 - val_mse: 258.7854\n",
      "Epoch 230/350\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 83.3175 - mse: 83.3175 - val_loss: 260.7493 - val_mse: 260.7493\n",
      "Epoch 231/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 82.5286 - mse: 82.5286 - val_loss: 257.0343 - val_mse: 257.0343\n",
      "Epoch 232/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 82.0930 - mse: 82.0930 - val_loss: 258.0028 - val_mse: 258.0028\n",
      "Epoch 233/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 81.8027 - mse: 81.8027 - val_loss: 255.8724 - val_mse: 255.8724\n",
      "Epoch 234/350\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 82.4269 - mse: 82.4269 - val_loss: 254.0578 - val_mse: 254.0578\n",
      "Epoch 235/350\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 80.6265 - mse: 80.6265 - val_loss: 256.6505 - val_mse: 256.6505\n",
      "Epoch 236/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 80.9238 - mse: 80.9238 - val_loss: 259.2471 - val_mse: 259.2470\n",
      "Epoch 237/350\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 80.2086 - mse: 80.2086 - val_loss: 254.8236 - val_mse: 254.8236\n",
      "Epoch 238/350\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 79.8656 - mse: 79.8656 - val_loss: 254.3922 - val_mse: 254.3922\n",
      "Epoch 239/350\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 80.4565 - mse: 80.4565 - val_loss: 253.2633 - val_mse: 253.2633\n",
      "Epoch 240/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 79.4133 - mse: 79.4133 - val_loss: 256.7751 - val_mse: 256.7751\n",
      "Epoch 241/350\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 79.5570 - mse: 79.5570 - val_loss: 255.7955 - val_mse: 255.7955\n",
      "Epoch 242/350\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 79.5117 - mse: 79.5117 - val_loss: 253.6709 - val_mse: 253.6709\n",
      "Epoch 243/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 78.6164 - mse: 78.6164 - val_loss: 255.1445 - val_mse: 255.1445\n",
      "Epoch 244/350\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 78.6177 - mse: 78.6177 - val_loss: 255.5306 - val_mse: 255.5306\n",
      "Epoch 245/350\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 78.7283 - mse: 78.7283 - val_loss: 255.5850 - val_mse: 255.5850\n",
      "Epoch 246/350\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 77.6449 - mse: 77.6449 - val_loss: 253.9465 - val_mse: 253.9465\n",
      "Epoch 247/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 78.0755 - mse: 78.0755 - val_loss: 254.0247 - val_mse: 254.0247\n",
      "Epoch 248/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 77.2083 - mse: 77.2083 - val_loss: 254.4908 - val_mse: 254.4908\n",
      "Epoch 249/350\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 76.9966 - mse: 76.9966 - val_loss: 253.2326 - val_mse: 253.2326\n",
      "Epoch 250/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 77.3420 - mse: 77.3420 - val_loss: 252.8662 - val_mse: 252.8663\n",
      "Epoch 251/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 76.8590 - mse: 76.8590 - val_loss: 251.5162 - val_mse: 251.5162\n",
      "Epoch 252/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 76.2758 - mse: 76.2758 - val_loss: 253.9298 - val_mse: 253.9298\n",
      "Epoch 253/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 76.4559 - mse: 76.4559 - val_loss: 252.5532 - val_mse: 252.5532\n",
      "Epoch 254/350\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 75.8603 - mse: 75.8603 - val_loss: 254.3401 - val_mse: 254.3401\n",
      "Epoch 255/350\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 76.1825 - mse: 76.1825 - val_loss: 253.6821 - val_mse: 253.6821\n",
      "Epoch 256/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 76.2420 - mse: 76.2420 - val_loss: 251.6356 - val_mse: 251.6356\n",
      "Epoch 257/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 75.2467 - mse: 75.2467 - val_loss: 254.8721 - val_mse: 254.8721\n",
      "Epoch 258/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 75.0984 - mse: 75.0984 - val_loss: 252.7875 - val_mse: 252.7875\n",
      "Epoch 259/350\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 74.7954 - mse: 74.7954 - val_loss: 251.3599 - val_mse: 251.3599\n",
      "Epoch 260/350\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 74.7634 - mse: 74.7634 - val_loss: 250.9910 - val_mse: 250.9910\n",
      "Epoch 261/350\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 73.9523 - mse: 73.9523 - val_loss: 252.1711 - val_mse: 252.1711\n",
      "Epoch 262/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 74.8317 - mse: 74.8317 - val_loss: 250.5808 - val_mse: 250.5808\n",
      "Epoch 263/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 74.2540 - mse: 74.2540 - val_loss: 249.4116 - val_mse: 249.4116\n",
      "Epoch 264/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 73.8931 - mse: 73.8931 - val_loss: 250.6196 - val_mse: 250.6196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function IteratorResourceDeleter.__del__ at 0x000002A27FC43048>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\", line 546, in __del__\n",
      "    handle=self._handle, deleter=self._deleter)\n",
      "  File \"C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\", line 1263, in delete_iterator\n",
      "    _ctx, \"DeleteIterator\", name, handle, deleter)\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 265/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 73.6698 - mse: 73.6698 - val_loss: 252.3033 - val_mse: 252.3033\n",
      "Epoch 266/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 73.5537 - mse: 73.5537 - val_loss: 249.4819 - val_mse: 249.4819\n",
      "Epoch 267/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 74.9207 - mse: 74.9207 - val_loss: 250.2741 - val_mse: 250.2741\n",
      "Epoch 268/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 73.5815 - mse: 73.5815 - val_loss: 248.5703 - val_mse: 248.5703\n",
      "Epoch 269/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 72.4200 - mse: 72.4200 - val_loss: 248.4611 - val_mse: 248.4611\n",
      "Epoch 270/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 73.3535 - mse: 73.3535 - val_loss: 248.9030 - val_mse: 248.9030\n",
      "Epoch 271/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 74.2251 - mse: 74.2251 - val_loss: 247.4475 - val_mse: 247.4475\n",
      "Epoch 272/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 72.1469 - mse: 72.1469 - val_loss: 248.6312 - val_mse: 248.6312\n",
      "Epoch 273/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 71.4950 - mse: 71.4950 - val_loss: 248.7554 - val_mse: 248.7554\n",
      "Epoch 274/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 71.2726 - mse: 71.2726 - val_loss: 247.7110 - val_mse: 247.7110\n",
      "Epoch 275/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 71.2074 - mse: 71.2074 - val_loss: 249.2212 - val_mse: 249.2212\n",
      "Epoch 276/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 71.1367 - mse: 71.1367 - val_loss: 248.0605 - val_mse: 248.0605\n",
      "Epoch 277/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 70.9268 - mse: 70.9268 - val_loss: 248.5755 - val_mse: 248.5755\n",
      "Epoch 278/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 70.1987 - mse: 70.1987 - val_loss: 247.3524 - val_mse: 247.3524\n",
      "Epoch 279/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 70.2309 - mse: 70.2309 - val_loss: 246.6863 - val_mse: 246.6863\n",
      "Epoch 280/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 69.6841 - mse: 69.6841 - val_loss: 247.9114 - val_mse: 247.9114\n",
      "Epoch 281/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 70.0577 - mse: 70.0577 - val_loss: 246.7433 - val_mse: 246.7433\n",
      "Epoch 282/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 69.8062 - mse: 69.8062 - val_loss: 247.2349 - val_mse: 247.2349\n",
      "Epoch 283/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 69.2252 - mse: 69.2252 - val_loss: 246.1656 - val_mse: 246.1656\n",
      "Epoch 284/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 68.9263 - mse: 68.9263 - val_loss: 246.4715 - val_mse: 246.4715\n",
      "Epoch 285/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 69.3811 - mse: 69.3811 - val_loss: 247.2085 - val_mse: 247.2085\n",
      "Epoch 286/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 69.0008 - mse: 69.0008 - val_loss: 246.2681 - val_mse: 246.2681\n",
      "Epoch 287/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 68.9248 - mse: 68.9248 - val_loss: 247.6507 - val_mse: 247.6507\n",
      "Epoch 288/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 67.9258 - mse: 67.9258 - val_loss: 245.5522 - val_mse: 245.5522\n",
      "Epoch 289/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 68.2935 - mse: 68.2935 - val_loss: 244.4088 - val_mse: 244.4088\n",
      "Epoch 290/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 67.6571 - mse: 67.6571 - val_loss: 245.0696 - val_mse: 245.0696\n",
      "Epoch 291/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 68.7510 - mse: 68.7510 - val_loss: 247.8984 - val_mse: 247.8984\n",
      "Epoch 292/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 67.8411 - mse: 67.8411 - val_loss: 244.0090 - val_mse: 244.0090\n",
      "Epoch 293/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 67.4229 - mse: 67.4229 - val_loss: 244.7009 - val_mse: 244.7009\n",
      "Epoch 294/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 66.8582 - mse: 66.8582 - val_loss: 244.1292 - val_mse: 244.1292\n",
      "Epoch 295/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 66.4027 - mse: 66.4027 - val_loss: 246.4034 - val_mse: 246.4034\n",
      "Epoch 296/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 66.5397 - mse: 66.5397 - val_loss: 245.7639 - val_mse: 245.7639\n",
      "Epoch 297/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 66.2806 - mse: 66.2805 - val_loss: 243.6122 - val_mse: 243.6122\n",
      "Epoch 298/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 66.4997 - mse: 66.4997 - val_loss: 245.0721 - val_mse: 245.0721\n",
      "Epoch 299/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 65.9051 - mse: 65.9051 - val_loss: 244.7999 - val_mse: 244.7999\n",
      "Epoch 300/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 65.8517 - mse: 65.8517 - val_loss: 245.9585 - val_mse: 245.9585\n",
      "Epoch 301/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 65.2516 - mse: 65.2516 - val_loss: 244.3778 - val_mse: 244.3778\n",
      "Epoch 302/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 65.5123 - mse: 65.5123 - val_loss: 245.9683 - val_mse: 245.9683\n",
      "Epoch 303/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 65.1178 - mse: 65.1178 - val_loss: 244.7899 - val_mse: 244.7899\n",
      "Epoch 304/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 65.1412 - mse: 65.1412 - val_loss: 245.0930 - val_mse: 245.0930\n",
      "Epoch 305/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 65.2322 - mse: 65.2322 - val_loss: 245.7242 - val_mse: 245.7242\n",
      "Epoch 306/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 65.2457 - mse: 65.2457 - val_loss: 244.8832 - val_mse: 244.8832\n",
      "Epoch 307/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 64.7223 - mse: 64.7223 - val_loss: 247.0754 - val_mse: 247.0754\n",
      "Epoch 308/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 64.2553 - mse: 64.2553 - val_loss: 245.2963 - val_mse: 245.2963\n",
      "Epoch 309/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 64.3914 - mse: 64.3914 - val_loss: 243.4615 - val_mse: 243.4615\n",
      "Epoch 310/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 64.8613 - mse: 64.8613 - val_loss: 243.7814 - val_mse: 243.7814\n",
      "Epoch 311/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 63.4663 - mse: 63.4663 - val_loss: 243.6630 - val_mse: 243.6630\n",
      "Epoch 312/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 63.3883 - mse: 63.3883 - val_loss: 245.0367 - val_mse: 245.0367\n",
      "Epoch 313/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 63.4853 - mse: 63.4853 - val_loss: 244.7505 - val_mse: 244.7505\n",
      "Epoch 314/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 63.3458 - mse: 63.3458 - val_loss: 244.6831 - val_mse: 244.6831\n",
      "Epoch 315/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 63.2543 - mse: 63.2543 - val_loss: 244.9538 - val_mse: 244.9538\n",
      "Epoch 316/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 62.6129 - mse: 62.6129 - val_loss: 244.0352 - val_mse: 244.0352\n",
      "Epoch 317/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 63.1593 - mse: 63.1593 - val_loss: 244.0152 - val_mse: 244.0153\n",
      "Epoch 318/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 62.7166 - mse: 62.7166 - val_loss: 244.0227 - val_mse: 244.0227\n",
      "Epoch 319/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 63.2582 - mse: 63.2582 - val_loss: 246.8998 - val_mse: 246.8998\n",
      "Epoch 320/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 62.3375 - mse: 62.3375 - val_loss: 244.1894 - val_mse: 244.1894\n",
      "Epoch 321/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 62.4610 - mse: 62.4610 - val_loss: 244.4044 - val_mse: 244.4044\n",
      "Epoch 322/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 61.8305 - mse: 61.8305 - val_loss: 244.1847 - val_mse: 244.1847\n",
      "Epoch 323/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 61.6066 - mse: 61.6066 - val_loss: 245.8836 - val_mse: 245.8836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 324/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 61.6015 - mse: 61.6015 - val_loss: 245.3440 - val_mse: 245.3440\n",
      "Epoch 325/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 61.1074 - mse: 61.1074 - val_loss: 245.7842 - val_mse: 245.7842\n",
      "Epoch 326/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 60.9838 - mse: 60.9838 - val_loss: 245.6238 - val_mse: 245.6238\n",
      "Epoch 327/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 61.8882 - mse: 61.8882 - val_loss: 243.4550 - val_mse: 243.4550\n",
      "Epoch 328/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 61.2100 - mse: 61.2100 - val_loss: 244.2316 - val_mse: 244.2316\n",
      "Epoch 329/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 61.9576 - mse: 61.9576 - val_loss: 245.8186 - val_mse: 245.8186\n",
      "Epoch 330/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 60.9341 - mse: 60.9341 - val_loss: 246.5480 - val_mse: 246.5480\n",
      "Epoch 331/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 61.1308 - mse: 61.1308 - val_loss: 246.9308 - val_mse: 246.9308\n",
      "Epoch 332/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 60.3654 - mse: 60.3654 - val_loss: 247.8248 - val_mse: 247.8248\n",
      "Epoch 333/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 60.6182 - mse: 60.6182 - val_loss: 247.6860 - val_mse: 247.6860\n",
      "Epoch 334/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 60.1023 - mse: 60.1023 - val_loss: 246.7701 - val_mse: 246.7701\n",
      "Epoch 335/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 59.9055 - mse: 59.9055 - val_loss: 244.0296 - val_mse: 244.0296\n",
      "Epoch 336/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 59.7670 - mse: 59.7670 - val_loss: 243.4278 - val_mse: 243.4278\n",
      "Epoch 337/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 59.2484 - mse: 59.2484 - val_loss: 245.5792 - val_mse: 245.5792\n",
      "Epoch 338/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 59.6989 - mse: 59.6989 - val_loss: 245.2658 - val_mse: 245.2658\n",
      "Epoch 339/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 60.1067 - mse: 60.1067 - val_loss: 244.6663 - val_mse: 244.6663\n",
      "Epoch 340/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 59.6582 - mse: 59.6582 - val_loss: 244.0309 - val_mse: 244.0309\n",
      "Epoch 341/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 58.5767 - mse: 58.5767 - val_loss: 244.9303 - val_mse: 244.9303\n",
      "Epoch 342/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 59.4727 - mse: 59.4727 - val_loss: 245.9820 - val_mse: 245.9820\n",
      "Epoch 343/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 58.6923 - mse: 58.6923 - val_loss: 245.1395 - val_mse: 245.1395\n",
      "Epoch 344/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 58.9262 - mse: 58.9262 - val_loss: 245.6000 - val_mse: 245.6000\n",
      "Epoch 345/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 58.6341 - mse: 58.6341 - val_loss: 245.2238 - val_mse: 245.2238\n",
      "Epoch 346/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 58.5585 - mse: 58.5585 - val_loss: 245.8425 - val_mse: 245.8425\n",
      "Epoch 347/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 59.1016 - mse: 59.1016 - val_loss: 246.5329 - val_mse: 246.5329\n",
      "Epoch 348/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 59.4211 - mse: 59.4211 - val_loss: 247.6307 - val_mse: 247.6307\n",
      "Epoch 349/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 59.4260 - mse: 59.4260 - val_loss: 246.0242 - val_mse: 246.0242\n",
      "Epoch 350/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 57.7545 - mse: 57.7545 - val_loss: 246.6236 - val_mse: 246.6236\n",
      "Epoch 1/350\n",
      "10/10 [==============================] - 1s 32ms/step - loss: 15979.0078 - mse: 15979.0078 - val_loss: 15962.3242 - val_mse: 15962.3252\n",
      "Epoch 2/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 15838.6416 - mse: 15838.6416 - val_loss: 15811.8125 - val_mse: 15811.8125\n",
      "Epoch 3/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 15661.5371 - mse: 15661.5371 - val_loss: 15604.1318 - val_mse: 15604.1318\n",
      "Epoch 4/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 15393.5107 - mse: 15393.5107 - val_loss: 15294.1914 - val_mse: 15294.1914\n",
      "Epoch 5/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 14995.4219 - mse: 14995.4219 - val_loss: 14822.8408 - val_mse: 14822.8408\n",
      "Epoch 6/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 14395.2803 - mse: 14395.2803 - val_loss: 14125.6787 - val_mse: 14125.6787\n",
      "Epoch 7/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 13511.1426 - mse: 13511.1426 - val_loss: 13126.9219 - val_mse: 13126.9219\n",
      "Epoch 8/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 12278.9854 - mse: 12278.9854 - val_loss: 11751.9062 - val_mse: 11751.9053\n",
      "Epoch 9/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 10635.6855 - mse: 10635.6855 - val_loss: 9983.6182 - val_mse: 9983.6182\n",
      "Epoch 10/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 8627.3662 - mse: 8627.3662 - val_loss: 7914.4165 - val_mse: 7914.4165\n",
      "Epoch 11/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 6467.8384 - mse: 6467.8384 - val_loss: 5853.7104 - val_mse: 5853.7104\n",
      "Epoch 12/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 4556.5371 - mse: 4556.5371 - val_loss: 4226.0259 - val_mse: 4226.0259\n",
      "Epoch 13/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 3321.0210 - mse: 3321.0210 - val_loss: 3233.5872 - val_mse: 3233.5872\n",
      "Epoch 14/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 2651.2886 - mse: 2651.2886 - val_loss: 2742.5671 - val_mse: 2742.5671\n",
      "Epoch 15/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 2350.1404 - mse: 2350.1404 - val_loss: 2493.7529 - val_mse: 2493.7529\n",
      "Epoch 16/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 2168.3254 - mse: 2168.3254 - val_loss: 2389.7610 - val_mse: 2389.7610\n",
      "Epoch 17/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 2055.4475 - mse: 2055.4475 - val_loss: 2297.0967 - val_mse: 2297.0967\n",
      "Epoch 18/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1957.7563 - mse: 1957.7563 - val_loss: 2257.6382 - val_mse: 2257.6382\n",
      "Epoch 19/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 1876.7682 - mse: 1876.7682 - val_loss: 2167.6487 - val_mse: 2167.6487\n",
      "Epoch 20/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1798.9020 - mse: 1798.9020 - val_loss: 2114.4268 - val_mse: 2114.4268\n",
      "Epoch 21/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1719.2390 - mse: 1719.2393 - val_loss: 2031.6801 - val_mse: 2031.6801\n",
      "Epoch 22/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1660.7112 - mse: 1660.7112 - val_loss: 1933.1559 - val_mse: 1933.1559\n",
      "Epoch 23/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1587.6327 - mse: 1587.6327 - val_loss: 1914.7706 - val_mse: 1914.7706\n",
      "Epoch 24/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1521.1665 - mse: 1521.1665 - val_loss: 1868.8278 - val_mse: 1868.8278\n",
      "Epoch 25/350\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 1468.9519 - mse: 1468.9519 - val_loss: 1809.0367 - val_mse: 1809.0367\n",
      "Epoch 26/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1405.5289 - mse: 1405.5289 - val_loss: 1754.5117 - val_mse: 1754.5117\n",
      "Epoch 27/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 1355.2100 - mse: 1355.2100 - val_loss: 1702.4580 - val_mse: 1702.4580\n",
      "Epoch 28/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1306.1393 - mse: 1306.1393 - val_loss: 1652.1484 - val_mse: 1652.1484\n",
      "Epoch 29/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 1259.3575 - mse: 1259.3575 - val_loss: 1618.4164 - val_mse: 1618.4164\n",
      "Epoch 30/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1213.8752 - mse: 1213.8752 - val_loss: 1597.0929 - val_mse: 1597.0929\n",
      "Epoch 31/350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 5ms/step - loss: 1169.1429 - mse: 1169.1429 - val_loss: 1560.9525 - val_mse: 1560.9525\n",
      "Epoch 32/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 1126.3599 - mse: 1126.3599 - val_loss: 1507.8232 - val_mse: 1507.8232\n",
      "Epoch 33/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 1088.0271 - mse: 1088.0271 - val_loss: 1439.3575 - val_mse: 1439.3575\n",
      "Epoch 34/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 1053.7639 - mse: 1053.7639 - val_loss: 1418.9716 - val_mse: 1418.9716\n",
      "Epoch 35/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 1013.0856 - mse: 1013.0856 - val_loss: 1373.7552 - val_mse: 1373.7552\n",
      "Epoch 36/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 977.3765 - mse: 977.3765 - val_loss: 1339.5614 - val_mse: 1339.5614\n",
      "Epoch 37/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 940.3493 - mse: 940.3493 - val_loss: 1303.8651 - val_mse: 1303.8651\n",
      "Epoch 38/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 908.7334 - mse: 908.7335 - val_loss: 1255.8301 - val_mse: 1255.8301\n",
      "Epoch 39/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 878.4260 - mse: 878.4260 - val_loss: 1242.3994 - val_mse: 1242.3994\n",
      "Epoch 40/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 845.3409 - mse: 845.3409 - val_loss: 1204.8298 - val_mse: 1204.8298\n",
      "Epoch 41/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 817.6069 - mse: 817.6069 - val_loss: 1156.3873 - val_mse: 1156.3873\n",
      "Epoch 42/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 791.7020 - mse: 791.7020 - val_loss: 1142.4231 - val_mse: 1142.4230\n",
      "Epoch 43/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 763.0255 - mse: 763.0255 - val_loss: 1086.1014 - val_mse: 1086.1014\n",
      "Epoch 44/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 736.4904 - mse: 736.4904 - val_loss: 1068.3135 - val_mse: 1068.3135\n",
      "Epoch 45/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 708.9501 - mse: 708.9501 - val_loss: 1035.2616 - val_mse: 1035.2616\n",
      "Epoch 46/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 683.2010 - mse: 683.2010 - val_loss: 1011.3478 - val_mse: 1011.3478\n",
      "Epoch 47/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 657.7239 - mse: 657.7239 - val_loss: 974.6612 - val_mse: 974.6612\n",
      "Epoch 48/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 635.6163 - mse: 635.6163 - val_loss: 945.6633 - val_mse: 945.6633\n",
      "Epoch 49/350\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 611.9908 - mse: 611.9908 - val_loss: 928.9987 - val_mse: 928.9987\n",
      "Epoch 50/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 593.2761 - mse: 593.2761 - val_loss: 903.8757 - val_mse: 903.8757\n",
      "Epoch 51/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 574.2908 - mse: 574.2908 - val_loss: 885.8960 - val_mse: 885.8960\n",
      "Epoch 52/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 557.9924 - mse: 557.9924 - val_loss: 864.0679 - val_mse: 864.0679\n",
      "Epoch 53/350\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 540.7126 - mse: 540.7126 - val_loss: 815.2073 - val_mse: 815.2073\n",
      "Epoch 54/350\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 422.9483 - mse: 422.9483"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10540/2477291209.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m                     \u001b[0mdirectory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'TfTuner'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m                     )\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mtuner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_trans\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m350\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_validate_trans\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_validate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mbest_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_best_models\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras_tuner\\engine\\base_tuner.py\u001b[0m in \u001b[0;36msearch\u001b[1;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_trial_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m             \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_trial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m             \u001b[1;31m# `results` is None indicates user updated oracle in `run_trial()`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresults\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras_tuner\\engine\\tuner.py\u001b[0m in \u001b[0;36mrun_trial\u001b[1;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[0;32m    302\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_checkpoint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m             \u001b[0mcopied_kwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"callbacks\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 304\u001b[1;33m             \u001b[0mobj_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build_and_fit_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcopied_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    305\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m             \u001b[1;31m# objective left unspecified,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras_tuner\\engine\\tuner.py\u001b[0m in \u001b[0;36m_build_and_fit_model\u001b[1;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[0mhp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_try_build\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhypermodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrun_trial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras_tuner\\engine\\hypermodel.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, hp, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    135\u001b[0m             \u001b[0mIf\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[0ma\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mit\u001b[0m \u001b[0mshould\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m         \"\"\"\n\u001b[1;32m--> 137\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1223\u001b[0m               \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1224\u001b[0m               \u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1225\u001b[1;33m               _use_cached_eval_dataset=True)\n\u001b[0m\u001b[0;32m   1226\u001b[0m           \u001b[0mval_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'val_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mval\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mval_logs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1227\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   1481\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_test_counter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1482\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_test_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1483\u001b[1;33m       \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Single epoch.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1484\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1485\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36menumerate_epochs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1197\u001b[0m     \u001b[1;34m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1198\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_truncate_execution_to_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1199\u001b[1;33m       \u001b[0mdata_iterator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1200\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initial_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1201\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    485\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 486\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    487\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m       raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[0;32m    694\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcomponents\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0melement_spec\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 696\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    697\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_create_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    717\u001b[0m               \u001b[0moutput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m               output_shapes=self._flat_output_shapes))\n\u001b[1;32m--> 719\u001b[1;33m       \u001b[0mgen_dataset_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    720\u001b[0m       \u001b[1;31m# Delete the resource when this object is deleted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m       self._resource_deleter = IteratorResourceDeleter(\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[1;34m(dataset, iterator, name)\u001b[0m\n\u001b[0;32m   3118\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3119\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[1;32m-> 3120\u001b[1;33m         _ctx, \"MakeIterator\", name, dataset, iterator)\n\u001b[0m\u001b[0;32m   3121\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3122\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#run the tuner to find the best model\n",
    "tuner = kt.RandomSearch(buildModelTF,\n",
    "                     objective='val_mse',\n",
    "                     max_trials=30,\n",
    "                        executions_per_trial=4,\n",
    "                    overwrite = False,\n",
    "                    directory='TfTuner'\n",
    "                    )\n",
    "tuner.search(X_train_trans, Y_train, epochs=350, batch_size=20,validation_data=(X_validate_trans, Y_validate))\n",
    "best_model = tuner.get_best_models()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dbc322",
   "metadata": {},
   "source": [
    "After trialing both the transformed and the subsetted datasets for best parameters and validating, I find the subsetted dataset to be the one that consistently performs best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "441b0b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run final check with the selected parameters\n",
    "modelTF = keras.Sequential()\n",
    "modelTF.add(layers.Dense(2, input_dim=24, activation=\"relu\"))\n",
    "modelTF.add(layers.Dense(150,activation=\"relu\"))\n",
    "modelTF.add(layers.Dense(150, activation=\"relu\"))\n",
    "modelTF.add(layers.Dense(1))\n",
    "keras.optimizers.Adam(learning_rate=0.00006)\n",
    "modelTF.compile(loss='mean_squared_error',optimizer='adam',metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "3d6b32cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set early stopping condition\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "82627585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/510\n",
      "10/10 [==============================] - 1s 31ms/step - loss: 15978.9961 - mse: 15978.9961 - val_loss: 15965.3945 - val_mse: 15965.3945\n",
      "Epoch 2/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 15822.0049 - mse: 15822.0049 - val_loss: 15781.9912 - val_mse: 15781.9893\n",
      "Epoch 3/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 15563.8506 - mse: 15563.8506 - val_loss: 15464.2266 - val_mse: 15464.2266\n",
      "Epoch 4/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 15090.7539 - mse: 15090.7539 - val_loss: 14948.2109 - val_mse: 14948.2109\n",
      "Epoch 5/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 14364.8105 - mse: 14364.8105 - val_loss: 14136.2744 - val_mse: 14136.2744\n",
      "Epoch 6/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 13227.4775 - mse: 13227.4775 - val_loss: 12988.2305 - val_mse: 12988.2305\n",
      "Epoch 7/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 11657.1475 - mse: 11657.1475 - val_loss: 11593.9482 - val_mse: 11593.9482\n",
      "Epoch 8/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 9953.3467 - mse: 9953.3467 - val_loss: 10203.9688 - val_mse: 10203.9688\n",
      "Epoch 9/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 8393.2188 - mse: 8393.2188 - val_loss: 9249.1318 - val_mse: 9249.1318\n",
      "Epoch 10/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 7433.6367 - mse: 7433.6367 - val_loss: 8578.7705 - val_mse: 8578.7705\n",
      "Epoch 11/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 6885.2168 - mse: 6885.2168 - val_loss: 7895.1133 - val_mse: 7895.1133\n",
      "Epoch 12/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 6217.6265 - mse: 6217.6265 - val_loss: 7171.6304 - val_mse: 7171.6304\n",
      "Epoch 13/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 5613.6519 - mse: 5613.6519 - val_loss: 6472.2280 - val_mse: 6472.2280\n",
      "Epoch 14/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 5041.7036 - mse: 5041.7036 - val_loss: 5786.6353 - val_mse: 5786.6353\n",
      "Epoch 15/510\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 4471.2437 - mse: 4471.2437 - val_loss: 5096.0317 - val_mse: 5096.0317\n",
      "Epoch 16/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 3918.3718 - mse: 3918.3718 - val_loss: 4403.8188 - val_mse: 4403.8188\n",
      "Epoch 17/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 3391.1653 - mse: 3391.1653 - val_loss: 3750.6980 - val_mse: 3750.6980\n",
      "Epoch 18/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 2865.1892 - mse: 2865.1892 - val_loss: 3150.1897 - val_mse: 3150.1897\n",
      "Epoch 19/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 2387.2832 - mse: 2387.2832 - val_loss: 2603.6313 - val_mse: 2603.6313\n",
      "Epoch 20/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1937.3099 - mse: 1937.3099 - val_loss: 2137.5168 - val_mse: 2137.5168\n",
      "Epoch 21/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1566.7439 - mse: 1566.7439 - val_loss: 1690.4022 - val_mse: 1690.4022\n",
      "Epoch 22/510\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 1248.1750 - mse: 1248.1750 - val_loss: 1347.3080 - val_mse: 1347.3080\n",
      "Epoch 23/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 968.4931 - mse: 968.4931 - val_loss: 1065.5195 - val_mse: 1065.5197\n",
      "Epoch 24/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 761.7808 - mse: 761.7808 - val_loss: 856.5665 - val_mse: 856.5665\n",
      "Epoch 25/510\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 606.6804 - mse: 606.6804 - val_loss: 698.7166 - val_mse: 698.7166\n",
      "Epoch 26/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 493.8969 - mse: 493.8969 - val_loss: 584.7221 - val_mse: 584.7221\n",
      "Epoch 27/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 418.7279 - mse: 418.7279 - val_loss: 502.6941 - val_mse: 502.6941\n",
      "Epoch 28/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 368.1355 - mse: 368.1355 - val_loss: 449.8669 - val_mse: 449.8669\n",
      "Epoch 29/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 332.9567 - mse: 332.9567 - val_loss: 414.7186 - val_mse: 414.7186\n",
      "Epoch 30/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 313.8437 - mse: 313.8437 - val_loss: 390.4400 - val_mse: 390.4400\n",
      "Epoch 31/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 301.4006 - mse: 301.4006 - val_loss: 374.9038 - val_mse: 374.9038\n",
      "Epoch 32/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 293.8004 - mse: 293.8004 - val_loss: 365.4931 - val_mse: 365.4931\n",
      "Epoch 33/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 288.8485 - mse: 288.8485 - val_loss: 358.5846 - val_mse: 358.5846\n",
      "Epoch 34/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 284.8575 - mse: 284.8575 - val_loss: 352.3224 - val_mse: 352.3224\n",
      "Epoch 35/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 279.6573 - mse: 279.6573 - val_loss: 346.2230 - val_mse: 346.2230\n",
      "Epoch 36/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 277.6008 - mse: 277.6008 - val_loss: 341.8496 - val_mse: 341.8496\n",
      "Epoch 37/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 275.0689 - mse: 275.0689 - val_loss: 338.9977 - val_mse: 338.9977\n",
      "Epoch 38/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 272.0227 - mse: 272.0227 - val_loss: 336.5939 - val_mse: 336.5939\n",
      "Epoch 39/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 269.2600 - mse: 269.2600 - val_loss: 333.8076 - val_mse: 333.8076\n",
      "Epoch 40/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 267.1656 - mse: 267.1656 - val_loss: 330.6844 - val_mse: 330.6844\n",
      "Epoch 41/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 265.0642 - mse: 265.0642 - val_loss: 328.2679 - val_mse: 328.2679\n",
      "Epoch 42/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 265.9033 - mse: 265.9033 - val_loss: 327.5333 - val_mse: 327.5334\n",
      "Epoch 43/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 260.0396 - mse: 260.0396 - val_loss: 324.1424 - val_mse: 324.1424\n",
      "Epoch 44/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 258.5969 - mse: 258.5969 - val_loss: 322.5548 - val_mse: 322.5548\n",
      "Epoch 45/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 256.8914 - mse: 256.8914 - val_loss: 320.9393 - val_mse: 320.9393\n",
      "Epoch 46/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 255.5651 - mse: 255.5651 - val_loss: 319.3960 - val_mse: 319.3960\n",
      "Epoch 47/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 253.2019 - mse: 253.2019 - val_loss: 316.0887 - val_mse: 316.0887\n",
      "Epoch 48/510\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 252.2825 - mse: 252.2825 - val_loss: 314.2298 - val_mse: 314.2298\n",
      "Epoch 49/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 251.4246 - mse: 251.4246 - val_loss: 312.4010 - val_mse: 312.4010\n",
      "Epoch 50/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 248.7902 - mse: 248.7902 - val_loss: 312.8546 - val_mse: 312.8546\n",
      "Epoch 51/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 247.6947 - mse: 247.6947 - val_loss: 310.6619 - val_mse: 310.6619\n",
      "Epoch 52/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 245.5595 - mse: 245.5595 - val_loss: 308.0144 - val_mse: 308.0144\n",
      "Epoch 53/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 244.9383 - mse: 244.9383 - val_loss: 305.6249 - val_mse: 305.6249\n",
      "Epoch 54/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 243.7728 - mse: 243.7728 - val_loss: 305.6349 - val_mse: 305.6349\n",
      "Epoch 55/510\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 241.5979 - mse: 241.5979 - val_loss: 303.5260 - val_mse: 303.5260\n",
      "Epoch 56/510\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 240.9051 - mse: 240.9051 - val_loss: 302.8128 - val_mse: 302.8128\n",
      "Epoch 57/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 240.9626 - mse: 240.9626 - val_loss: 298.8756 - val_mse: 298.8756\n",
      "Epoch 58/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 237.5936 - mse: 237.5936 - val_loss: 299.8352 - val_mse: 299.8352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 240.2986 - mse: 240.2986 - val_loss: 299.7750 - val_mse: 299.7750\n",
      "Epoch 60/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 237.3190 - mse: 237.3190 - val_loss: 293.8994 - val_mse: 293.8994\n",
      "Epoch 61/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 235.6189 - mse: 235.6189 - val_loss: 293.4165 - val_mse: 293.4165\n",
      "Epoch 62/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 233.5053 - mse: 233.5053 - val_loss: 293.0732 - val_mse: 293.0732\n",
      "Epoch 63/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 233.8002 - mse: 233.8002 - val_loss: 293.0688 - val_mse: 293.0688\n",
      "Epoch 64/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 232.3240 - mse: 232.3240 - val_loss: 288.8232 - val_mse: 288.8232\n",
      "Epoch 65/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 231.3223 - mse: 231.3223 - val_loss: 288.1298 - val_mse: 288.1298\n",
      "Epoch 66/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 229.8461 - mse: 229.8461 - val_loss: 288.3936 - val_mse: 288.3936\n",
      "Epoch 67/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 229.6742 - mse: 229.6742 - val_loss: 287.3527 - val_mse: 287.3527\n",
      "Epoch 68/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 228.9573 - mse: 228.9573 - val_loss: 285.6443 - val_mse: 285.6443\n",
      "Epoch 69/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 228.2357 - mse: 228.2357 - val_loss: 282.0482 - val_mse: 282.0482\n",
      "Epoch 70/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 227.6854 - mse: 227.6854 - val_loss: 281.8905 - val_mse: 281.8905\n",
      "Epoch 71/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 229.1447 - mse: 229.1447 - val_loss: 286.0689 - val_mse: 286.0689\n",
      "Epoch 72/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 226.5324 - mse: 226.5324 - val_loss: 278.9891 - val_mse: 278.9891\n",
      "Epoch 73/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 225.9662 - mse: 225.9662 - val_loss: 279.5636 - val_mse: 279.5636\n",
      "Epoch 74/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 224.2379 - mse: 224.2379 - val_loss: 279.5274 - val_mse: 279.5274\n",
      "Epoch 75/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 223.3542 - mse: 223.3542 - val_loss: 278.0854 - val_mse: 278.0854\n",
      "Epoch 76/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 222.0922 - mse: 222.0922 - val_loss: 275.6560 - val_mse: 275.6560\n",
      "Epoch 77/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 221.6334 - mse: 221.6334 - val_loss: 274.9063 - val_mse: 274.9063\n",
      "Epoch 78/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 220.8611 - mse: 220.8611 - val_loss: 275.3362 - val_mse: 275.3362\n",
      "Epoch 79/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 219.9305 - mse: 219.9305 - val_loss: 272.6659 - val_mse: 272.6659\n",
      "Epoch 80/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 219.6276 - mse: 219.6276 - val_loss: 270.3359 - val_mse: 270.3359\n",
      "Epoch 81/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 218.9415 - mse: 218.9415 - val_loss: 272.8328 - val_mse: 272.8328\n",
      "Epoch 82/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 220.6828 - mse: 220.6828 - val_loss: 269.0479 - val_mse: 269.0479\n",
      "Epoch 83/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 216.6232 - mse: 216.6232 - val_loss: 270.2942 - val_mse: 270.2942\n",
      "Epoch 84/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 216.0978 - mse: 216.0978 - val_loss: 268.2039 - val_mse: 268.2039\n",
      "Epoch 85/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 215.5527 - mse: 215.5527 - val_loss: 266.3014 - val_mse: 266.3014\n",
      "Epoch 86/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 214.9816 - mse: 214.9816 - val_loss: 263.9816 - val_mse: 263.9816\n",
      "Epoch 87/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 214.7209 - mse: 214.7209 - val_loss: 265.2061 - val_mse: 265.2061\n",
      "Epoch 88/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 215.1619 - mse: 215.1619 - val_loss: 261.8731 - val_mse: 261.8731\n",
      "Epoch 89/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 212.0521 - mse: 212.0521 - val_loss: 266.0536 - val_mse: 266.0536\n",
      "Epoch 90/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 214.1078 - mse: 214.1078 - val_loss: 263.5277 - val_mse: 263.5277\n",
      "Epoch 91/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 211.1383 - mse: 211.1383 - val_loss: 258.0275 - val_mse: 258.0275\n",
      "Epoch 92/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 213.3862 - mse: 213.3862 - val_loss: 257.7289 - val_mse: 257.7289\n",
      "Epoch 93/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 211.5325 - mse: 211.5325 - val_loss: 261.4854 - val_mse: 261.4854\n",
      "Epoch 94/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 210.3349 - mse: 210.3349 - val_loss: 255.9416 - val_mse: 255.9416\n",
      "Epoch 95/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 209.6071 - mse: 209.6071 - val_loss: 255.3465 - val_mse: 255.3465\n",
      "Epoch 96/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 210.4009 - mse: 210.4009 - val_loss: 252.4916 - val_mse: 252.4916\n",
      "Epoch 97/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 209.2615 - mse: 209.2615 - val_loss: 255.5258 - val_mse: 255.5258\n",
      "Epoch 98/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 208.2487 - mse: 208.2487 - val_loss: 257.9221 - val_mse: 257.9221\n",
      "Epoch 99/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 207.7632 - mse: 207.7632 - val_loss: 252.0662 - val_mse: 252.0662\n",
      "Epoch 100/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 207.1115 - mse: 207.1115 - val_loss: 252.6385 - val_mse: 252.6385\n",
      "Epoch 101/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 206.5672 - mse: 206.5672 - val_loss: 253.1922 - val_mse: 253.1922\n",
      "Epoch 102/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 208.3082 - mse: 208.3082 - val_loss: 247.4544 - val_mse: 247.4544\n",
      "Epoch 103/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 204.7570 - mse: 204.7570 - val_loss: 251.7990 - val_mse: 251.7990\n",
      "Epoch 104/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 204.5880 - mse: 204.5880 - val_loss: 249.0070 - val_mse: 249.0070\n",
      "Epoch 105/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 204.4122 - mse: 204.4122 - val_loss: 245.0304 - val_mse: 245.0304\n",
      "Epoch 106/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 203.3758 - mse: 203.3758 - val_loss: 244.3320 - val_mse: 244.3320\n",
      "Epoch 107/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 202.8815 - mse: 202.8815 - val_loss: 245.3927 - val_mse: 245.3927\n",
      "Epoch 108/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 204.4631 - mse: 204.4631 - val_loss: 241.7938 - val_mse: 241.7938\n",
      "Epoch 109/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 201.5685 - mse: 201.5685 - val_loss: 243.4587 - val_mse: 243.4587\n",
      "Epoch 110/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 200.8952 - mse: 200.8952 - val_loss: 243.4835 - val_mse: 243.4835\n",
      "Epoch 111/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 202.1029 - mse: 202.1029 - val_loss: 242.7016 - val_mse: 242.7016\n",
      "Epoch 112/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 201.9047 - mse: 201.9047 - val_loss: 237.5218 - val_mse: 237.5218\n",
      "Epoch 113/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 200.5350 - mse: 200.5350 - val_loss: 241.7307 - val_mse: 241.7307\n",
      "Epoch 114/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 198.9632 - mse: 198.9632 - val_loss: 239.2208 - val_mse: 239.2208\n",
      "Epoch 115/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 198.7745 - mse: 198.7745 - val_loss: 235.1179 - val_mse: 235.1179\n",
      "Epoch 116/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 200.5186 - mse: 200.5186 - val_loss: 238.9154 - val_mse: 238.9154\n",
      "Epoch 117/510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 5ms/step - loss: 198.2262 - mse: 198.2262 - val_loss: 237.9437 - val_mse: 237.9437\n",
      "Epoch 118/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 197.1653 - mse: 197.1653 - val_loss: 234.7999 - val_mse: 234.7999\n",
      "Epoch 119/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 196.8302 - mse: 196.8302 - val_loss: 232.2521 - val_mse: 232.2521\n",
      "Epoch 120/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 196.8268 - mse: 196.8268 - val_loss: 236.0335 - val_mse: 236.0335\n",
      "Epoch 121/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 201.0169 - mse: 201.0169 - val_loss: 230.1418 - val_mse: 230.1417\n",
      "Epoch 122/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 204.6300 - mse: 204.6300 - val_loss: 242.2093 - val_mse: 242.2093\n",
      "Epoch 123/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 196.0078 - mse: 196.0078 - val_loss: 227.2596 - val_mse: 227.2596\n",
      "Epoch 124/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 196.3963 - mse: 196.3963 - val_loss: 230.8801 - val_mse: 230.8801\n",
      "Epoch 125/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 194.7847 - mse: 194.7847 - val_loss: 228.2594 - val_mse: 228.2594\n",
      "Epoch 126/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 194.4127 - mse: 194.4127 - val_loss: 229.9964 - val_mse: 229.9964\n",
      "Epoch 127/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 191.7871 - mse: 191.7871 - val_loss: 226.1931 - val_mse: 226.1931\n",
      "Epoch 128/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 191.3192 - mse: 191.3192 - val_loss: 226.7150 - val_mse: 226.7150\n",
      "Epoch 129/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 191.2863 - mse: 191.2863 - val_loss: 228.2652 - val_mse: 228.2652\n",
      "Epoch 130/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 194.0247 - mse: 194.0247 - val_loss: 228.1481 - val_mse: 228.1481\n",
      "Epoch 131/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 192.7126 - mse: 192.7126 - val_loss: 221.7683 - val_mse: 221.7683\n",
      "Epoch 132/510\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 189.9056 - mse: 189.9056 - val_loss: 225.9285 - val_mse: 225.9285\n",
      "Epoch 133/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 189.2006 - mse: 189.2006 - val_loss: 223.0509 - val_mse: 223.0509\n",
      "Epoch 134/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 189.1863 - mse: 189.1863 - val_loss: 222.3253 - val_mse: 222.3253\n",
      "Epoch 135/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 188.4217 - mse: 188.4217 - val_loss: 225.1038 - val_mse: 225.1038\n",
      "Epoch 136/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 188.7253 - mse: 188.7253 - val_loss: 221.5352 - val_mse: 221.5352\n",
      "Epoch 137/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 187.5094 - mse: 187.5094 - val_loss: 220.8200 - val_mse: 220.8200\n",
      "Epoch 138/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 189.8866 - mse: 189.8866 - val_loss: 218.3903 - val_mse: 218.3903\n",
      "Epoch 139/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 185.2905 - mse: 185.2905 - val_loss: 224.0678 - val_mse: 224.0678\n",
      "Epoch 140/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 187.5757 - mse: 187.5757 - val_loss: 218.8315 - val_mse: 218.8315\n",
      "Epoch 141/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 185.7764 - mse: 185.7764 - val_loss: 216.1837 - val_mse: 216.1837\n",
      "Epoch 142/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 185.3205 - mse: 185.3205 - val_loss: 216.7641 - val_mse: 216.7641\n",
      "Epoch 143/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 184.7021 - mse: 184.7021 - val_loss: 217.8818 - val_mse: 217.8818\n",
      "Epoch 144/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 188.7414 - mse: 188.7414 - val_loss: 219.6362 - val_mse: 219.6362\n",
      "Epoch 145/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 189.2833 - mse: 189.2833 - val_loss: 212.6284 - val_mse: 212.6284\n",
      "Epoch 146/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 184.7080 - mse: 184.7080 - val_loss: 219.4681 - val_mse: 219.4681\n",
      "Epoch 147/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 182.8732 - mse: 182.8732 - val_loss: 212.1197 - val_mse: 212.1197\n",
      "Epoch 148/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 183.0377 - mse: 183.0377 - val_loss: 212.8055 - val_mse: 212.8055\n",
      "Epoch 149/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 184.0788 - mse: 184.0788 - val_loss: 213.9749 - val_mse: 213.9749\n",
      "Epoch 150/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 182.3296 - mse: 182.3296 - val_loss: 210.2014 - val_mse: 210.2014\n",
      "Epoch 151/510\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 182.1009 - mse: 182.1009 - val_loss: 212.8417 - val_mse: 212.8417\n",
      "Epoch 152/510\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 181.4863 - mse: 181.4863 - val_loss: 211.1661 - val_mse: 211.1661\n",
      "Epoch 153/510\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 181.7428 - mse: 181.7428 - val_loss: 208.8390 - val_mse: 208.8390\n",
      "Epoch 154/510\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 180.1790 - mse: 180.1790 - val_loss: 211.0018 - val_mse: 211.0018\n",
      "Epoch 155/510\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 179.9624 - mse: 179.9624 - val_loss: 209.9426 - val_mse: 209.9426\n",
      "Epoch 156/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 179.5621 - mse: 179.5621 - val_loss: 208.6288 - val_mse: 208.6288\n",
      "Epoch 157/510\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 179.4158 - mse: 179.4158 - val_loss: 207.6138 - val_mse: 207.6138\n",
      "Epoch 158/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 180.8620 - mse: 180.8620 - val_loss: 205.8779 - val_mse: 205.8779\n",
      "Epoch 159/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 178.5861 - mse: 178.5861 - val_loss: 209.5876 - val_mse: 209.5876\n",
      "Epoch 160/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 178.3016 - mse: 178.3016 - val_loss: 205.2427 - val_mse: 205.2427\n",
      "Epoch 161/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 177.8692 - mse: 177.8692 - val_loss: 204.0260 - val_mse: 204.0260\n",
      "Epoch 162/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 180.2302 - mse: 180.2302 - val_loss: 204.7828 - val_mse: 204.7828\n",
      "Epoch 163/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 190.6959 - mse: 190.6959 - val_loss: 212.6664 - val_mse: 212.6664\n",
      "Epoch 164/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 179.7162 - mse: 179.7162 - val_loss: 201.0033 - val_mse: 201.0033\n",
      "Epoch 165/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 182.4099 - mse: 182.4099 - val_loss: 212.4036 - val_mse: 212.4036\n",
      "Epoch 166/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 177.6095 - mse: 177.6095 - val_loss: 201.5255 - val_mse: 201.5255\n",
      "Epoch 167/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 179.4818 - mse: 179.4818 - val_loss: 201.4267 - val_mse: 201.4267\n",
      "Epoch 168/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 177.6617 - mse: 177.6617 - val_loss: 208.6000 - val_mse: 208.6000\n",
      "Epoch 169/510\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 175.6953 - mse: 175.6953 - val_loss: 200.9047 - val_mse: 200.9047\n",
      "Epoch 170/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 174.7533 - mse: 174.7533 - val_loss: 200.7533 - val_mse: 200.7533\n",
      "Epoch 171/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 174.4481 - mse: 174.4481 - val_loss: 201.2237 - val_mse: 201.2237\n",
      "Epoch 172/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 174.1175 - mse: 174.1175 - val_loss: 200.0294 - val_mse: 200.0294\n",
      "Epoch 173/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 173.5731 - mse: 173.5731 - val_loss: 199.7108 - val_mse: 199.7108\n",
      "Epoch 174/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 173.2776 - mse: 173.2776 - val_loss: 199.5610 - val_mse: 199.5609\n",
      "Epoch 175/510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 7ms/step - loss: 174.8545 - mse: 174.8545 - val_loss: 202.4416 - val_mse: 202.4416\n",
      "Epoch 176/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 173.8050 - mse: 173.8050 - val_loss: 196.0332 - val_mse: 196.0332\n",
      "Epoch 177/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 172.3848 - mse: 172.3848 - val_loss: 201.3312 - val_mse: 201.3312\n",
      "Epoch 178/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 173.1829 - mse: 173.1829 - val_loss: 199.9764 - val_mse: 199.9764\n",
      "Epoch 179/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 171.9121 - mse: 171.9121 - val_loss: 196.2305 - val_mse: 196.2305\n",
      "Epoch 180/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 174.5335 - mse: 174.5335 - val_loss: 198.3836 - val_mse: 198.3836\n",
      "Epoch 181/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 173.5177 - mse: 173.5177 - val_loss: 193.7362 - val_mse: 193.7362\n",
      "Epoch 182/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 172.9338 - mse: 172.9338 - val_loss: 197.9142 - val_mse: 197.9142\n",
      "Epoch 183/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 171.6025 - mse: 171.6025 - val_loss: 194.7464 - val_mse: 194.7464\n",
      "Epoch 184/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 170.6183 - mse: 170.6183 - val_loss: 196.2477 - val_mse: 196.2477\n",
      "Epoch 185/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 173.9195 - mse: 173.9195 - val_loss: 193.1736 - val_mse: 193.1736\n",
      "Epoch 186/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 167.9867 - mse: 167.9867 - val_loss: 201.3491 - val_mse: 201.3491\n",
      "Epoch 187/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 171.3756 - mse: 171.3756 - val_loss: 192.5957 - val_mse: 192.5957\n",
      "Epoch 188/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 169.4394 - mse: 169.4394 - val_loss: 194.6434 - val_mse: 194.6434\n",
      "Epoch 189/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 168.8027 - mse: 168.8027 - val_loss: 192.5155 - val_mse: 192.5155\n",
      "Epoch 190/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 170.8037 - mse: 170.8037 - val_loss: 194.2667 - val_mse: 194.2666\n",
      "Epoch 191/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 167.3478 - mse: 167.3478 - val_loss: 192.1008 - val_mse: 192.1008\n",
      "Epoch 192/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 169.9234 - mse: 169.9234 - val_loss: 190.3913 - val_mse: 190.3913\n",
      "Epoch 193/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 170.4808 - mse: 170.4808 - val_loss: 195.0408 - val_mse: 195.0408\n",
      "Epoch 194/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 165.3168 - mse: 165.3168 - val_loss: 190.3157 - val_mse: 190.3157\n",
      "Epoch 195/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 166.3985 - mse: 166.3985 - val_loss: 192.3100 - val_mse: 192.3100\n",
      "Epoch 196/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 164.9794 - mse: 164.9794 - val_loss: 190.6202 - val_mse: 190.6202\n",
      "Epoch 197/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 164.9779 - mse: 164.9779 - val_loss: 191.3497 - val_mse: 191.3497\n",
      "Epoch 198/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 163.9997 - mse: 163.9997 - val_loss: 189.6652 - val_mse: 189.6652\n",
      "Epoch 199/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 164.8452 - mse: 164.8452 - val_loss: 188.9951 - val_mse: 188.9951\n",
      "Epoch 200/510\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 165.5347 - mse: 165.5347 - val_loss: 190.9843 - val_mse: 190.9843\n",
      "Epoch 201/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 163.4389 - mse: 163.4389 - val_loss: 187.6658 - val_mse: 187.6658\n",
      "Epoch 202/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 163.5055 - mse: 163.5055 - val_loss: 189.8362 - val_mse: 189.8362\n",
      "Epoch 203/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 165.4385 - mse: 165.4385 - val_loss: 187.1287 - val_mse: 187.1287\n",
      "Epoch 204/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 162.1995 - mse: 162.1995 - val_loss: 196.2343 - val_mse: 196.2343\n",
      "Epoch 205/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 162.9831 - mse: 162.9831 - val_loss: 186.4597 - val_mse: 186.4597\n",
      "Epoch 206/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 163.5413 - mse: 163.5413 - val_loss: 187.8806 - val_mse: 187.8806\n",
      "Epoch 207/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 164.0482 - mse: 164.0482 - val_loss: 188.6160 - val_mse: 188.6160\n",
      "Epoch 208/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 160.5635 - mse: 160.5635 - val_loss: 185.4665 - val_mse: 185.4665\n",
      "Epoch 209/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 162.3667 - mse: 162.3667 - val_loss: 186.7972 - val_mse: 186.7972\n",
      "Epoch 210/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 163.3581 - mse: 163.3581 - val_loss: 184.9262 - val_mse: 184.9262\n",
      "Epoch 211/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 160.2551 - mse: 160.2551 - val_loss: 187.6606 - val_mse: 187.6606\n",
      "Epoch 212/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 161.2479 - mse: 161.2479 - val_loss: 185.6945 - val_mse: 185.6945\n",
      "Epoch 213/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 161.8163 - mse: 161.8163 - val_loss: 188.6509 - val_mse: 188.6509\n",
      "Epoch 214/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 162.0402 - mse: 162.0402 - val_loss: 182.6936 - val_mse: 182.6936\n",
      "Epoch 215/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 162.6676 - mse: 162.6676 - val_loss: 190.7305 - val_mse: 190.7305\n",
      "Epoch 216/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 160.1460 - mse: 160.1460 - val_loss: 181.5716 - val_mse: 181.5716\n",
      "Epoch 217/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 159.2749 - mse: 159.2749 - val_loss: 184.8649 - val_mse: 184.8649\n",
      "Epoch 218/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 159.0063 - mse: 159.0063 - val_loss: 183.4474 - val_mse: 183.4474\n",
      "Epoch 219/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 157.4913 - mse: 157.4913 - val_loss: 181.9727 - val_mse: 181.9727\n",
      "Epoch 220/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 157.0559 - mse: 157.0559 - val_loss: 184.3445 - val_mse: 184.3445\n",
      "Epoch 221/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 159.2073 - mse: 159.2073 - val_loss: 180.9972 - val_mse: 180.9972\n",
      "Epoch 222/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 155.4723 - mse: 155.4723 - val_loss: 184.7160 - val_mse: 184.7160\n",
      "Epoch 223/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 156.3961 - mse: 156.3961 - val_loss: 181.4276 - val_mse: 181.4276\n",
      "Epoch 224/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 155.5074 - mse: 155.5074 - val_loss: 178.1600 - val_mse: 178.1600\n",
      "Epoch 225/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 156.0421 - mse: 156.0421 - val_loss: 180.6092 - val_mse: 180.6092\n",
      "Epoch 226/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 156.5011 - mse: 156.5011 - val_loss: 177.5205 - val_mse: 177.5205\n",
      "Epoch 227/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 155.2073 - mse: 155.2073 - val_loss: 178.5712 - val_mse: 178.5712\n",
      "Epoch 228/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 154.3810 - mse: 154.3810 - val_loss: 177.4806 - val_mse: 177.4806\n",
      "Epoch 229/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 153.1417 - mse: 153.1417 - val_loss: 177.8871 - val_mse: 177.8871\n",
      "Epoch 230/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 153.1883 - mse: 153.1883 - val_loss: 178.5679 - val_mse: 178.5679\n",
      "Epoch 231/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 153.7459 - mse: 153.7459 - val_loss: 175.4501 - val_mse: 175.4501\n",
      "Epoch 232/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 152.4812 - mse: 152.4812 - val_loss: 175.2854 - val_mse: 175.2854\n",
      "Epoch 233/510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 7ms/step - loss: 152.5548 - mse: 152.5548 - val_loss: 175.5666 - val_mse: 175.5666\n",
      "Epoch 234/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 153.1941 - mse: 153.1941 - val_loss: 174.9489 - val_mse: 174.9489\n",
      "Epoch 235/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 152.4550 - mse: 152.4550 - val_loss: 174.5501 - val_mse: 174.5501\n",
      "Epoch 236/510\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 153.3540 - mse: 153.3540 - val_loss: 172.9668 - val_mse: 172.9668\n",
      "Epoch 237/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 151.6102 - mse: 151.6102 - val_loss: 172.5341 - val_mse: 172.5341\n",
      "Epoch 238/510\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 150.6917 - mse: 150.6917 - val_loss: 171.1591 - val_mse: 171.1591\n",
      "Epoch 239/510\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 150.2849 - mse: 150.2849 - val_loss: 172.8365 - val_mse: 172.8365\n",
      "Epoch 240/510\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 150.8707 - mse: 150.8707 - val_loss: 171.3028 - val_mse: 171.3028\n",
      "Epoch 241/510\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 150.1927 - mse: 150.1927 - val_loss: 171.4079 - val_mse: 171.4079\n",
      "Epoch 242/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 148.3391 - mse: 148.3391 - val_loss: 168.9507 - val_mse: 168.9507\n",
      "Epoch 243/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 148.9998 - mse: 148.9998 - val_loss: 170.0210 - val_mse: 170.0210\n",
      "Epoch 244/510\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 149.4268 - mse: 149.4268 - val_loss: 170.3118 - val_mse: 170.3118\n",
      "Epoch 245/510\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 150.0339 - mse: 150.0339 - val_loss: 167.2316 - val_mse: 167.2316\n",
      "Epoch 246/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 149.7196 - mse: 149.7196 - val_loss: 171.6025 - val_mse: 171.6025\n",
      "Epoch 247/510\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 148.5664 - mse: 148.5664 - val_loss: 166.9483 - val_mse: 166.9483\n",
      "Epoch 248/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 147.2191 - mse: 147.2191 - val_loss: 167.6450 - val_mse: 167.6450\n",
      "Epoch 249/510\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 147.0569 - mse: 147.0569 - val_loss: 166.6968 - val_mse: 166.6968\n",
      "Epoch 250/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 146.8956 - mse: 146.8956 - val_loss: 165.6769 - val_mse: 165.6769\n",
      "Epoch 251/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 146.5632 - mse: 146.5632 - val_loss: 165.8447 - val_mse: 165.8447\n",
      "Epoch 252/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 146.7536 - mse: 146.7536 - val_loss: 166.3474 - val_mse: 166.3474\n",
      "Epoch 253/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 148.9186 - mse: 148.9186 - val_loss: 167.8871 - val_mse: 167.8871\n",
      "Epoch 254/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 152.0105 - mse: 152.0105 - val_loss: 163.4248 - val_mse: 163.4248\n",
      "Epoch 255/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 147.1344 - mse: 147.1344 - val_loss: 166.9358 - val_mse: 166.9358\n",
      "Epoch 256/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 144.7794 - mse: 144.7794 - val_loss: 162.7821 - val_mse: 162.7821\n",
      "Epoch 257/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 144.8182 - mse: 144.8182 - val_loss: 163.9138 - val_mse: 163.9138\n",
      "Epoch 258/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 143.9180 - mse: 143.9180 - val_loss: 162.9964 - val_mse: 162.9964\n",
      "Epoch 259/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 143.3332 - mse: 143.3332 - val_loss: 161.7648 - val_mse: 161.7648\n",
      "Epoch 260/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 143.5944 - mse: 143.5944 - val_loss: 162.5459 - val_mse: 162.5459\n",
      "Epoch 261/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 143.7301 - mse: 143.7301 - val_loss: 161.8368 - val_mse: 161.8368\n",
      "Epoch 262/510\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 142.5988 - mse: 142.5988 - val_loss: 161.5475 - val_mse: 161.5475\n",
      "Epoch 263/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 142.5111 - mse: 142.5111 - val_loss: 162.0684 - val_mse: 162.0684\n",
      "Epoch 264/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 141.5907 - mse: 141.5907 - val_loss: 159.7447 - val_mse: 159.7447\n",
      "Epoch 265/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 142.3542 - mse: 142.3542 - val_loss: 161.3722 - val_mse: 161.3722\n",
      "Epoch 266/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 141.1671 - mse: 141.1671 - val_loss: 159.4640 - val_mse: 159.4640\n",
      "Epoch 267/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 142.0319 - mse: 142.0319 - val_loss: 159.3484 - val_mse: 159.3484\n",
      "Epoch 268/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 141.4834 - mse: 141.4834 - val_loss: 159.8486 - val_mse: 159.8486\n",
      "Epoch 269/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 140.0076 - mse: 140.0076 - val_loss: 158.3799 - val_mse: 158.3799\n",
      "Epoch 270/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 140.3454 - mse: 140.3454 - val_loss: 158.9099 - val_mse: 158.9099\n",
      "Epoch 271/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 140.7944 - mse: 140.7944 - val_loss: 159.1303 - val_mse: 159.1303\n",
      "Epoch 272/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 139.5039 - mse: 139.5039 - val_loss: 156.7070 - val_mse: 156.7070\n",
      "Epoch 273/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 140.3160 - mse: 140.3160 - val_loss: 157.9951 - val_mse: 157.9951\n",
      "Epoch 274/510\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 140.3392 - mse: 140.3392 - val_loss: 156.0166 - val_mse: 156.0166\n",
      "Epoch 275/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 139.1884 - mse: 139.1884 - val_loss: 159.5596 - val_mse: 159.5596\n",
      "Epoch 276/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 140.2451 - mse: 140.2451 - val_loss: 155.6631 - val_mse: 155.6631\n",
      "Epoch 277/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 136.7318 - mse: 136.7318 - val_loss: 157.3365 - val_mse: 157.3365\n",
      "Epoch 278/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 138.1386 - mse: 138.1386 - val_loss: 155.1127 - val_mse: 155.1127\n",
      "Epoch 279/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 136.5526 - mse: 136.5526 - val_loss: 155.7247 - val_mse: 155.7247\n",
      "Epoch 280/510\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 136.0611 - mse: 136.0611 - val_loss: 154.2485 - val_mse: 154.2485\n",
      "Epoch 281/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 136.6942 - mse: 136.6942 - val_loss: 154.0181 - val_mse: 154.0181\n",
      "Epoch 282/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 135.8857 - mse: 135.8857 - val_loss: 153.4005 - val_mse: 153.4005\n",
      "Epoch 283/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 135.1201 - mse: 135.1201 - val_loss: 155.5963 - val_mse: 155.5963\n",
      "Epoch 284/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 133.9212 - mse: 133.9212 - val_loss: 152.7910 - val_mse: 152.7910\n",
      "Epoch 285/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 134.8318 - mse: 134.8318 - val_loss: 152.1039 - val_mse: 152.1039\n",
      "Epoch 286/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 134.5257 - mse: 134.5257 - val_loss: 153.9295 - val_mse: 153.9295\n",
      "Epoch 287/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 132.7811 - mse: 132.7811 - val_loss: 152.1392 - val_mse: 152.1392\n",
      "Epoch 288/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 133.2778 - mse: 133.2778 - val_loss: 151.4818 - val_mse: 151.4818\n",
      "Epoch 289/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 133.8803 - mse: 133.8803 - val_loss: 153.3068 - val_mse: 153.3068\n",
      "Epoch 290/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 131.9714 - mse: 131.9714 - val_loss: 150.9212 - val_mse: 150.9212\n",
      "Epoch 291/510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 5ms/step - loss: 131.4955 - mse: 131.4955 - val_loss: 151.3620 - val_mse: 151.3620\n",
      "Epoch 292/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 130.5349 - mse: 130.5349 - val_loss: 150.3523 - val_mse: 150.3523\n",
      "Epoch 293/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 134.6658 - mse: 134.6658 - val_loss: 150.3656 - val_mse: 150.3656\n",
      "Epoch 294/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 132.4277 - mse: 132.4277 - val_loss: 149.6945 - val_mse: 149.6945\n",
      "Epoch 295/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 132.4461 - mse: 132.4461 - val_loss: 152.0261 - val_mse: 152.0261\n",
      "Epoch 296/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 132.9429 - mse: 132.9429 - val_loss: 148.9602 - val_mse: 148.9602\n",
      "Epoch 297/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 129.5070 - mse: 129.5070 - val_loss: 149.8929 - val_mse: 149.8929\n",
      "Epoch 298/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 136.2465 - mse: 136.2465 - val_loss: 149.8481 - val_mse: 149.8481\n",
      "Epoch 299/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 132.8997 - mse: 132.8997 - val_loss: 148.7290 - val_mse: 148.7290\n",
      "Epoch 300/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 129.6570 - mse: 129.6570 - val_loss: 150.1969 - val_mse: 150.1969\n",
      "Epoch 301/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 128.1151 - mse: 128.1151 - val_loss: 148.1195 - val_mse: 148.1195\n",
      "Epoch 302/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 128.2373 - mse: 128.2373 - val_loss: 147.8781 - val_mse: 147.8781\n",
      "Epoch 303/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 127.8353 - mse: 127.8353 - val_loss: 146.9735 - val_mse: 146.9735\n",
      "Epoch 304/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 128.3828 - mse: 128.3828 - val_loss: 146.7078 - val_mse: 146.7078\n",
      "Epoch 305/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 128.1067 - mse: 128.1067 - val_loss: 148.3689 - val_mse: 148.3689\n",
      "Epoch 306/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 128.4832 - mse: 128.4832 - val_loss: 146.8378 - val_mse: 146.8378\n",
      "Epoch 307/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 126.9166 - mse: 126.9166 - val_loss: 147.5763 - val_mse: 147.5763\n",
      "Epoch 308/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 127.3272 - mse: 127.3272 - val_loss: 147.3160 - val_mse: 147.3160\n",
      "Epoch 309/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 126.4799 - mse: 126.4799 - val_loss: 146.0590 - val_mse: 146.0590\n",
      "Epoch 310/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 127.7967 - mse: 127.7967 - val_loss: 145.5954 - val_mse: 145.5954\n",
      "Epoch 311/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 126.3412 - mse: 126.3412 - val_loss: 146.4449 - val_mse: 146.4449\n",
      "Epoch 312/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 125.9970 - mse: 125.9970 - val_loss: 145.8812 - val_mse: 145.8812\n",
      "Epoch 313/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 125.3319 - mse: 125.3319 - val_loss: 145.2211 - val_mse: 145.2211\n",
      "Epoch 314/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 125.4116 - mse: 125.4116 - val_loss: 146.0271 - val_mse: 146.0271\n",
      "Epoch 315/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 126.4393 - mse: 126.4393 - val_loss: 145.0156 - val_mse: 145.0156\n",
      "Epoch 316/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 124.7048 - mse: 124.7048 - val_loss: 144.6823 - val_mse: 144.6823\n",
      "Epoch 317/510\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 124.9054 - mse: 124.9054 - val_loss: 144.1942 - val_mse: 144.1942\n",
      "Epoch 318/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 125.5990 - mse: 125.5990 - val_loss: 145.1590 - val_mse: 145.1590\n",
      "Epoch 319/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 125.4117 - mse: 125.4117 - val_loss: 143.2220 - val_mse: 143.2220\n",
      "Epoch 320/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 123.7523 - mse: 123.7523 - val_loss: 144.4073 - val_mse: 144.4073\n",
      "Epoch 321/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 124.8175 - mse: 124.8175 - val_loss: 143.8161 - val_mse: 143.8161\n",
      "Epoch 322/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 124.1554 - mse: 124.1554 - val_loss: 142.9553 - val_mse: 142.9553\n",
      "Epoch 323/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 123.8932 - mse: 123.8932 - val_loss: 143.1772 - val_mse: 143.1772\n",
      "Epoch 324/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 124.4844 - mse: 124.4844 - val_loss: 142.1493 - val_mse: 142.1493\n",
      "Epoch 325/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 123.0745 - mse: 123.0745 - val_loss: 141.7985 - val_mse: 141.7985\n",
      "Epoch 326/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 124.6975 - mse: 124.6975 - val_loss: 142.7250 - val_mse: 142.7250\n",
      "Epoch 327/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 123.3145 - mse: 123.3145 - val_loss: 141.9618 - val_mse: 141.9618\n",
      "Epoch 328/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 123.2116 - mse: 123.2116 - val_loss: 141.3194 - val_mse: 141.3194\n",
      "Epoch 329/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 122.9232 - mse: 122.9232 - val_loss: 140.7633 - val_mse: 140.7633\n",
      "Epoch 330/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 122.6133 - mse: 122.6133 - val_loss: 140.8537 - val_mse: 140.8537\n",
      "Epoch 331/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 122.2098 - mse: 122.2098 - val_loss: 139.9108 - val_mse: 139.9108\n",
      "Epoch 332/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 122.4232 - mse: 122.4232 - val_loss: 139.5405 - val_mse: 139.5405\n",
      "Epoch 333/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 121.6359 - mse: 121.6359 - val_loss: 139.7504 - val_mse: 139.7504\n",
      "Epoch 334/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 121.7968 - mse: 121.7968 - val_loss: 139.4552 - val_mse: 139.4552\n",
      "Epoch 335/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 122.7613 - mse: 122.7613 - val_loss: 140.1827 - val_mse: 140.1827\n",
      "Epoch 336/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 125.3677 - mse: 125.3677 - val_loss: 138.9959 - val_mse: 138.9959\n",
      "Epoch 337/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 121.9829 - mse: 121.9829 - val_loss: 139.9885 - val_mse: 139.9885\n",
      "Epoch 338/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 122.3306 - mse: 122.3306 - val_loss: 138.7095 - val_mse: 138.7095\n",
      "Epoch 339/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 121.0008 - mse: 121.0008 - val_loss: 138.6740 - val_mse: 138.6740\n",
      "Epoch 340/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 121.1150 - mse: 121.1150 - val_loss: 138.0601 - val_mse: 138.0601\n",
      "Epoch 341/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 122.0284 - mse: 122.0284 - val_loss: 137.3707 - val_mse: 137.3707\n",
      "Epoch 342/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 121.2929 - mse: 121.2929 - val_loss: 137.8063 - val_mse: 137.8063\n",
      "Epoch 343/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 122.8263 - mse: 122.8263 - val_loss: 137.2133 - val_mse: 137.2133\n",
      "Epoch 344/510\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 122.4688 - mse: 122.4688 - val_loss: 138.6973 - val_mse: 138.6973\n",
      "Epoch 345/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 120.6082 - mse: 120.6082 - val_loss: 137.8088 - val_mse: 137.8088\n",
      "Epoch 346/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 121.0920 - mse: 121.0920 - val_loss: 138.2195 - val_mse: 138.2195\n",
      "Epoch 347/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 121.8532 - mse: 121.8532 - val_loss: 136.3788 - val_mse: 136.3788\n",
      "Epoch 348/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 120.5594 - mse: 120.5594 - val_loss: 136.3339 - val_mse: 136.3338\n",
      "Epoch 349/510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 6ms/step - loss: 124.1995 - mse: 124.1995 - val_loss: 136.4140 - val_mse: 136.4140\n",
      "Epoch 350/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 122.3655 - mse: 122.3655 - val_loss: 136.5881 - val_mse: 136.5881\n",
      "Epoch 351/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 123.7723 - mse: 123.7723 - val_loss: 138.1859 - val_mse: 138.1859\n",
      "Epoch 352/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 123.1266 - mse: 123.1266 - val_loss: 137.2065 - val_mse: 137.2065\n",
      "Epoch 353/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 119.0338 - mse: 119.0338 - val_loss: 138.3784 - val_mse: 138.3784\n",
      "Epoch 354/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 119.8179 - mse: 119.8179 - val_loss: 135.9001 - val_mse: 135.9001\n",
      "Epoch 355/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 119.5223 - mse: 119.5223 - val_loss: 135.8484 - val_mse: 135.8484\n",
      "Epoch 356/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 120.0018 - mse: 120.0018 - val_loss: 137.4220 - val_mse: 137.4220\n",
      "Epoch 357/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 122.5703 - mse: 122.5703 - val_loss: 135.6434 - val_mse: 135.6434\n",
      "Epoch 358/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 122.8547 - mse: 122.8547 - val_loss: 135.5308 - val_mse: 135.5308\n",
      "Epoch 359/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 119.8390 - mse: 119.8390 - val_loss: 136.1964 - val_mse: 136.1964\n",
      "Epoch 360/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 118.7096 - mse: 118.7096 - val_loss: 135.6065 - val_mse: 135.6065\n",
      "Epoch 361/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 122.2946 - mse: 122.2946 - val_loss: 134.3238 - val_mse: 134.3238\n",
      "Epoch 362/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 120.6761 - mse: 120.6761 - val_loss: 135.1082 - val_mse: 135.1082\n",
      "Epoch 363/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 120.3405 - mse: 120.3405 - val_loss: 135.2480 - val_mse: 135.2480\n",
      "Epoch 364/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 119.9042 - mse: 119.9042 - val_loss: 135.1418 - val_mse: 135.1418\n",
      "Epoch 365/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 122.5847 - mse: 122.5847 - val_loss: 135.2247 - val_mse: 135.2247\n",
      "Epoch 366/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 118.8245 - mse: 118.8245 - val_loss: 135.2261 - val_mse: 135.2261\n",
      "Epoch 367/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 118.4694 - mse: 118.4694 - val_loss: 134.5701 - val_mse: 134.5701\n",
      "Epoch 368/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 118.2050 - mse: 118.2050 - val_loss: 134.5366 - val_mse: 134.5366\n",
      "Epoch 369/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 118.7387 - mse: 118.7387 - val_loss: 134.2251 - val_mse: 134.2251\n",
      "Epoch 370/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 118.7128 - mse: 118.7128 - val_loss: 134.2276 - val_mse: 134.2276\n",
      "Epoch 371/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 118.2758 - mse: 118.2758 - val_loss: 134.8171 - val_mse: 134.8171\n",
      "Epoch 372/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 118.1146 - mse: 118.1146 - val_loss: 134.1068 - val_mse: 134.1068\n",
      "Epoch 373/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 118.7061 - mse: 118.7061 - val_loss: 134.7940 - val_mse: 134.7940\n",
      "Epoch 374/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 117.8271 - mse: 117.8271 - val_loss: 134.4612 - val_mse: 134.4612\n",
      "Epoch 375/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 121.2665 - mse: 121.2665 - val_loss: 134.4608 - val_mse: 134.4608\n",
      "Epoch 376/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 117.5516 - mse: 117.5516 - val_loss: 134.4507 - val_mse: 134.4507\n",
      "Epoch 377/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 119.5306 - mse: 119.5306 - val_loss: 134.7093 - val_mse: 134.7093\n",
      "Epoch 378/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 117.1073 - mse: 117.1073 - val_loss: 134.2234 - val_mse: 134.2234\n",
      "Epoch 379/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 118.0314 - mse: 118.0314 - val_loss: 134.4530 - val_mse: 134.4530\n",
      "Epoch 380/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 117.3046 - mse: 117.3046 - val_loss: 134.8940 - val_mse: 134.8940\n",
      "Epoch 381/510\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 119.8197 - mse: 119.8197 - val_loss: 134.8859 - val_mse: 134.8859\n",
      "Epoch 382/510\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 122.1935 - mse: 122.1935 - val_loss: 134.5623 - val_mse: 134.5623\n"
     ]
    }
   ],
   "source": [
    "#Run model with best parameters\n",
    "history = modelTF.fit(X_train_subset_scaled, Y_train, epochs=510, batch_size=20, validation_data=(X_validate_subset_scaled, Y_validate), callbacks = [early_stopping_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "406dd700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  134.1067577775955  r2:  0.6628697215931373\n"
     ]
    }
   ],
   "source": [
    "#formally score the model\n",
    "pred = modelTF.predict(X_validate_subset_scaled)\n",
    "error2 = mean_squared_error(Y_validate, pred)\n",
    "r2 = r2_score(Y_validate, pred)\n",
    "print('MSE: ',error2, ' r2: ',r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab07459",
   "metadata": {},
   "source": [
    "The tensorflow model outperforms all of the others except the random forest with PCA.\n",
    "I will use the random forest with PCA and the subsetted data as the final model to test against the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc6c56c",
   "metadata": {},
   "source": [
    "## Model selection  \n",
    "Perform cross validation on top 3 models before making a selection  \n",
    "Top 3 models are:  \n",
    "* Random Forest with PCA -- subsetted data (r2: 68.87)  \n",
    "* Tensorflow with subsetted data (r2: 66.28)  \n",
    "* LassoCV with transformed, not subsetted, data (r2: 65.28)  \n",
    "\n",
    "Since I didn't cross-validate each of the models, I consider them all to be within roughly the same range. Both the random forest and tensorflow models are black box, not providing good visibility into how each of the features affects the cancer death rate. The LassoCV score is close to the others but also offers more explanatory power, since we can view the weight for each feature. Ideally, I would conduct the training again with cross-validation at each step. But given the current results, and given that the goal here is purely to test how well these community markers are *predictive* of cancer death rates, the top performer, random forest with PCA, is the model I'll select.  \n",
    "\n",
    "Note that, since this model uses the subsetted data, it does also shed light on which features best explain differences in cancer death rates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104cef91",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2b8a1d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Consolidate train and validation sets and retrain best models with this full set\n",
    "X_fullTrain = dropCat(train_data).drop('cancer_death_rate',axis=1)\n",
    "Y_fullTrain = train_data['cancer_death_rate']\n",
    "X_fullTrain_trans = pd.DataFrame(pt.fit_transform(X_fullTrain),columns=X_fullTrain.columns)\n",
    "X_fullTrain_subset_scaled = X_fullTrain_trans[list(table2['feature'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1ff6c92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetch and prep the test set\n",
    "test_data = pd.read_csv(\"communities_testing.csv\")\n",
    "X_test = dropCat(test_data).drop('cancer_death_rate',axis=1)\n",
    "Y_test = test_data['cancer_death_rate']\n",
    "X_test_trans = pd.DataFrame(pt.transform(X_test),columns=X_test.columns)\n",
    "X_test_subset_scaled = X_test_trans[list(table2['feature'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bfcc88ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  194.72773347720027  r2:  0.4706816710705217\n"
     ]
    }
   ],
   "source": [
    "#Run Random Forest with PCA model\n",
    "pca = PCA(n_components=.95)\n",
    "rf = RandomForestRegressor(max_depth=6, min_samples_leaf=2, random_state=43)\n",
    "pipe = make_pipeline(pca, rf)\n",
    "pipe.fit(X_fullTrain_subset_scaled, Y_fullTrain)\n",
    "rf_pred = pipe.predict(X_test_subset_scaled)\n",
    "r2 = r2_score(Y_test, rf_pred)\n",
    "error = mean_squared_error(Y_test, rf_pred)\n",
    "print('MSE: ',error, ' r2: ',r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32c800c",
   "metadata": {},
   "source": [
    "## Results  \n",
    "The final model exhibited an r-squared of .47.   \n",
    "As we can see below, the vast majority of predictions were accurate to within about 15%, with only nine of the 60 test cases off by 15 - 31% and one case off by 55%. \n",
    "The best prediction was off by less than 1%.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d70d0eed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Create dataframe with true rate and predicted rate and the difference between them.\n",
    "#Use histogram to view how the individual predictions did\n",
    "col_names = ['True_rate','RF_prediction']\n",
    "results = pd.DataFrame([Y_test,rf_pred]).T\n",
    "results.columns=col_names[0:2]\n",
    "results['diff'] = abs(results['True_rate']-results['RF_prediction'])\n",
    "results['percent'] = results['diff']/results['True_rate']*100\n",
    "plt.hist(results['percent'],bins=40)\n",
    "plt.title('Prediction inaccuracy')\n",
    "plt.xlabel('Percentage the prediction was off')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0233c9e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total predictions:  60\n",
      "predictions off > 15%:  9\n"
     ]
    }
   ],
   "source": [
    "#Get count of predictions > 15%\n",
    "print('total predictions: ', len(results))\n",
    "print('predictions off > 15%: ', len(results[results['percent']>15]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "19f658f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biggest miss:  55.35 %\n",
      "index:  6\n",
      "Area Code                         E09000020\n",
      "Area Name            Kensington and Chelsea\n",
      "gen_warts                           97.5367\n",
      "syphilis                             64.387\n",
      "gonorrhea                           379.309\n",
      "HIV                                 9.71101\n",
      "herpes                              100.087\n",
      "chlamydia                           561.633\n",
      "TB                                  11.7505\n",
      "comm_disease_flu                    7.56425\n",
      "antibiotics                        0.457252\n",
      "typhoid                            0.642092\n",
      "measles                             8.34719\n",
      "hepatitis                          0.642092\n",
      "STEC                                1.92065\n",
      "salmonella                          31.9028\n",
      "campylobacter                       40.8356\n",
      "giardia                             1.27611\n",
      "cryptosporidium                           0\n",
      "shigella                               5.74\n",
      "listeria                           0.640217\n",
      "mumps                               1.92065\n",
      "pertusis                            3.21046\n",
      "IMD                                 1.28418\n",
      "mental                              26.9974\n",
      "noise                               80.4335\n",
      "low_birth_wt                        2.51872\n",
      "cardiovascular                      50.2557\n",
      "respiratory                         22.5472\n",
      "osteoporosis                       0.247175\n",
      "indoor_air                           54.998\n",
      "RA                                 0.392709\n",
      "alcohol                             2111.87\n",
      "pollution                           12.0515\n",
      "inactive                            15.9435\n",
      "Five_a_day                          57.9986\n",
      "back_pain                           9.56134\n",
      "diabetes                            54.3051\n",
      "MSK                                  13.989\n",
      "walking                             27.6359\n",
      "biking                               4.3858\n",
      "cancer_death_rate                   69.2561\n",
      "education                            64.991\n",
      "premature                           88.6176\n",
      "parent_area                       E12000007\n",
      "parent_name                          London\n",
      "Name: 6, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Full record of the worst prediction\n",
    "biggest_miss = results['percent'].max()\n",
    "print('biggest miss: ',round(biggest_miss,2),'%')\n",
    "biggest_miss_idx = results[results['percent']==results['percent'].max()].index.values[0]\n",
    "print('index: ',biggest_miss_idx)\n",
    "print(test_data.loc[biggest_miss_idx,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "9536d958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "closest prediction:  0.42 %\n",
      "index:  56\n",
      "Area Code                  E07000127\n",
      "Area Name            West Lancashire\n",
      "gen_warts                    23.5816\n",
      "syphilis                     1.74679\n",
      "gonorrhea                    26.2018\n",
      "HIV                         0.600069\n",
      "herpes                        28.822\n",
      "chlamydia                    224.462\n",
      "TB                          0.584563\n",
      "comm_disease_flu             7.24606\n",
      "antibiotics                 0.812529\n",
      "typhoid                            0\n",
      "measles                            0\n",
      "hepatitis                          0\n",
      "STEC                               0\n",
      "salmonella                   5.29096\n",
      "campylobacter                116.401\n",
      "giardia                      11.4637\n",
      "cryptosporidium              12.3456\n",
      "shigella                        4.41\n",
      "listeria                           0\n",
      "mumps                         6.1431\n",
      "pertusis                     7.02488\n",
      "IMD                          1.75622\n",
      "mental                       21.6112\n",
      "noise                        4.92342\n",
      "low_birth_wt                 2.72628\n",
      "cardiovascular               81.1747\n",
      "respiratory                   42.327\n",
      "osteoporosis                0.767646\n",
      "indoor_air                    12.568\n",
      "RA                          0.857819\n",
      "alcohol                      2520.22\n",
      "pollution                      7.111\n",
      "inactive                      21.817\n",
      "Five_a_day                   51.7333\n",
      "back_pain                    10.6575\n",
      "diabetes                     77.0849\n",
      "MSK                           23.529\n",
      "walking                      16.9592\n",
      "biking                        1.6316\n",
      "cancer_death_rate            124.629\n",
      "education                    61.6048\n",
      "premature                    86.9279\n",
      "parent_area                E10000017\n",
      "parent_name               Lancashire\n",
      "Name: 56, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Full record of the best prediction\n",
    "closest_pred = results['percent'].min()\n",
    "print('closest prediction: ',round(closest_pred,2),'%')\n",
    "closest_pred_idx = results[results['percent']==results['percent'].min()].index.values[0]\n",
    "print('index: ',closest_pred_idx)\n",
    "print(test_data.loc[closest_pred_idx,:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
